{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5856eb82-bde0-4493-88c9-ea970f164923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989b0edb-449d-41a3-aba7-15a90451cd29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b5bb93-8576-458a-b3c3-78e14109a82a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/data/experiments/exp_siamese'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667b51b8-56f9-4a61-b295-49463cf00890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys                                                                             # Python system library needed to load custom functions\n",
    "import numpy as np                                                                     # for performing calculations on numerical arrays\n",
    "import pandas as pd                                                                    # home of the DataFrame construct, _the_ most important object for Data Science\n",
    "import seaborn as sns                                                                  # additional plotting library\n",
    "import matplotlib.pyplot as plt                                                        # allows creation of insightful plots\n",
    "import os                                                                              # for changing the directory\n",
    "\n",
    "import sagemaker                                                                       # dedicated sagemaker library to execute training jobs\n",
    "import boto3                                                                           # for interacting with S3 buckets\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace                                           # for executing the trainig jobs\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score             # tools to understand how our model is performing\n",
    "\n",
    "#sys.path.append('')                                                               # Add the source directory to the PYTHONPATH. This allows to import local functions and modules.\n",
    "from config import DEFAULT_BUCKET, DEFAULT_REGION  \n",
    "from gdsc_utils import create_encrypted_bucket, download_and_extract_model, PROJECT_DIR # functions to create S3 buckets and to help with downloading models. Importing our root directory\n",
    "from gdsc_eval import plot_confusion_matrix                                             # function for creating confusion matrix                                     # importing the bucket name that contains data for the challenge and the default region\n",
    "os.chdir(PROJECT_DIR)                                                                   # changing our directory to root\n",
    "\n",
    "\n",
    "import logging                                                    # module for displaying relevant information in the logs\n",
    "import sys                                                        # to access to some variables used or maintained by the interpreter \n",
    "import argparse                                                   # to parse arguments from passed in the hyperparameters\n",
    "import json                                                       # to open the json file with labels\n",
    "from transformers import (                                        # required classes to perform the model training and implement early stopping\n",
    "    ASTFeatureExtractor, \n",
    "    ASTForAudioClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    EarlyStoppingCallback\n",
    ")                                    \n",
    "import torch                                                       # library to work with PyTorch tensors and to figure out if we have a GPU available\n",
    "from datasets import load_dataset, Audio, Dataset                  # required tools to create, load and process our audio dataset\n",
    "from preprocessing import preprocess_audio_arrays                  # functions to preprocess the dataset with ASTFeatureExtractor\n",
    "from gdsc_eval import compute_metrics, make_predictions            # functions to create predictions and evaluate them\n",
    "from typing import Optional                                        # for type hints\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b49612-de96-4524-a0f1-44aff55ebc17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_extractor(model_name: str, \n",
    "                          train_dataset_mean: Optional[float] = None, \n",
    "                          train_dataset_std: Optional[float] = None) -> ASTFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Retrieves a feature extractor for audio signal processing.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained model to use.\n",
    "        train_dataset_mean (float, optional): The mean value of the training dataset. Defaults to None.\n",
    "        train_dataset_std (float, optional): The standard deviation of the training dataset. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        ASTFeatureExtractor: An instance of the ASTFeatureExtractor class.\n",
    "\n",
    "    \"\"\"\n",
    "    if all((train_dataset_mean, train_dataset_std)):\n",
    "        feature_extractor = ASTFeatureExtractor.from_pretrained(model_name, mean=train_dataset_mean, std=train_dataset_std, max_length=1024)\n",
    "        logger.info(f\" feature extractor loaded with dataset mean: {train_dataset_mean} and standard deviation: {train_dataset_std}\")\n",
    "    else:\n",
    "        feature_extractor = ASTFeatureExtractor.from_pretrained(model_name)\n",
    "        logger.info(\" at least one of the optional arguments (mean, std) is missing\")\n",
    "        logger.info(f\" feature extractor loaded with default dataset mean: {feature_extractor.mean} and standard deviation: {feature_extractor.std}\")\n",
    "        \n",
    "    return feature_extractor\n",
    "\n",
    "def preprocess_data_for_training(\n",
    "    dataset_path: str,\n",
    "    sampling_rate: int,\n",
    "    feature_extractor: ASTFeatureExtractor,\n",
    "    fe_batch_size: int,\n",
    "    dataset_name: str,\n",
    "    shuffle: bool = False,\n",
    "    extract_file_name: bool = True) -> Dataset:\n",
    "    \"\"\"\n",
    "    Preprocesses audio data for training.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): The path to the dataset.\n",
    "        sampling_rate (int): The desired sampling rate for the audio.\n",
    "        feature_extractor (ASTFeatureExtractor): The feature extractor to use for preprocessing.\n",
    "        fe_batch_size (int): The batch size for feature extraction.\n",
    "        dataset_name (str, optional): The name of the dataset. Defaults to None.\n",
    "        shuffle (bool, optional): Whether to shuffle the dataset. Defaults to False.\n",
    "        extract_file_name (bool, optional): Whether to extract paths from audio features. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dataset: The preprocessed dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"audiofolder\", data_dir=dataset_path).get('train') # loading the dataset\n",
    "    \n",
    "    # perform shuffle if specified\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(seed=42)\n",
    "        \n",
    "    logger.info(f\" loaded {dataset_name} dataset length is: {len(dataset)}\")\n",
    "\n",
    "    if extract_file_name:\n",
    "        remove_metadata = lambda x: x.endswith(\".wav\")\n",
    "        extract_file_name = lambda x: x.split('/')[-1]\n",
    "\n",
    "        dataset_paths = list(dataset.info.download_checksums.keys())\n",
    "        dataset_paths = list(filter(remove_metadata, dataset_paths))\n",
    "        dataset_paths = list(map(extract_file_name, dataset_paths))\n",
    "        dataset = dataset.add_column(\"file_name\", dataset_paths)\n",
    "\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "    \n",
    "    logger.info(f\" {dataset_name} dataset sampling rate casted to: {sampling_rate}\")\n",
    "\n",
    "    dataset_encoded = dataset.map(\n",
    "        lambda x: preprocess_audio_arrays(x, 'audio', 'array', feature_extractor),\n",
    "        remove_columns=\"audio\",\n",
    "        batched=True,\n",
    "        batch_size=fe_batch_size\n",
    "    )\n",
    "    \n",
    "    logger.info(f\" done extracting features for {dataset_name} dataset\")\n",
    "    \n",
    "    return dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55a254c-413d-4e0e-a143-d3561cb38b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# hyperparameters sent from our jupyter notebook are passed as command-line arguments to the script\n",
    "# preprocessing hyperparameters\n",
    "parser.add_argument(\"--sampling_rate\", type=int, default=22050)                        # sampling rate to which we will cast audio files\n",
    "parser.add_argument(\"--fe_batch_size\", type=int, default=32)                           # feature extractor batch size\n",
    "parser.add_argument(\"--train_dataset_mean\", type=float, default=-8.076275929131292)                  # mean value of spectrograms of our data\n",
    "parser.add_argument(\"--train_dataset_std\", type=float, default=3.984092920341275)                   # standard deviation value of spectrograms of our resampled data\n",
    "# parser.add_argument(\"--train_dataset_mean\", type=float, default=-8.141991150530815)                  # mean value of spectrograms of our data\n",
    "# parser.add_argument(\"--train_dataset_std\", type=float, default=4.095692486358449)                   # standard deviation value of spectrograms of our resampled data\n",
    "\n",
    "# training hyperparameters\n",
    "parser.add_argument(\"--model_name\", type=str)                                          # name of the pretrained model from HuggingFace\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=2e-5)                       # learning rate\n",
    "parser.add_argument(\"--epochs\", type=int, default=4)                                   # number of training epochs \n",
    "parser.add_argument(\"--train_batch_size\", type=int, default=4)                        # training batch size\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=64)                         # evaluation batch size\n",
    "parser.add_argument(\"--patience\", type=int, default=2)                                 # early stopping - how many epoch without improvement will stop the training \n",
    "# parser.add_argument(\"--data_channel\", type=str, default=os.environ[\"SM_CHANNEL_DATA\"]) # directory where input data from S3 is stored\n",
    "parser.add_argument(\"--train_dir\", type=str, default=\"train\")                          # folder name with training data\n",
    "parser.add_argument(\"--val_dir\", type=str, default=\"val\")                              # folder name with validation data\n",
    "parser.add_argument(\"--test_dir\", type=str, default=\"test\")                            # folder name with test data\n",
    "# parser.add_argument(\"--output_dir\", type=str, default=os.environ['SM_MODEL_DIR'])      # output directory. This directory will be saved in the S3 bucket\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()                    # parsing arguments from the notebook\n",
    "\n",
    "\n",
    "# train_path = f\"{args.data_channel}/{args.train_dir}\"   # directory of our training dataset on the instance\n",
    "# val_path = f\"{args.data_channel}/{args.val_dir}\"       # directory of our validation dataset on the instance\n",
    "# test_path = f\"{args.data_channel}/{args.test_dir}\"     # directory of our test dataset on the instance\n",
    "\n",
    "# Set up logging which allows to print information in logs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.getLevelName(\"INFO\"),\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1ee0c-1c6a-4d7d-b0fa-28b660bc3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/data_small/train'\n",
    "val_path = 'data/data_small/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "691f49e4-69bd-45c7-b9f2-12fbcd0f0302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'data/data_small/labels.json', 'r') as f:\n",
    "        labels = json.load(f)\n",
    "    \n",
    "# Create mapping from label to id and id to label\n",
    "label2id, id2label = dict(), dict()\n",
    "for k, v in labels.items():\n",
    "    label2id[k] = str(v)\n",
    "    id2label[str(v)] = k\n",
    "\n",
    "num_labels = len(label2id)  # define number of labels\n",
    "output_dir='models/AST',                # directory for saving model checkpoints and logs\n",
    "args.model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24699f62-1a13-4587-bd7b-d9b4cdc8dac9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bd5318f15d4cf0b0c4dd3f78e1266c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30 11:04:31,541 - __main__ - INFO -  feature extractor loaded with dataset mean: -8.076275929131292 and standard deviation: 3.984092920341275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75485e76b9f44d0b85c109d039a73875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-4ea904e8a4f39112/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf94b3914104dfe820247a3fb5cada8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf823b2120614eecb0eaaeeb8cea600e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30b190dce7746969c241d51eec1e709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-4ea904e8a4f39112/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b076cb03c640fd926d3106e5871a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30 11:04:31,963 - __main__ - INFO -  loaded train dataset length is: 176\n",
      "2023-06-30 11:04:31,966 - __main__ - INFO -  train dataset sampling rate casted to: 22050\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30 11:04:49,635 - __main__ - INFO -  done extracting features for train dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c12ad767d647ee9efe00ac897e1073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-99123276a0f5ab0b/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46873dbfece3409fa8ee6ae700f4257c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00308ac36c849c98717f51e444e72d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04e4d0f7a3e412c9adb549eb569de3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-99123276a0f5ab0b/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34548c00845742139f300445c1930e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30 11:04:49,909 - __main__ - INFO -  loaded validation dataset length is: 66\n",
      "2023-06-30 11:04:49,915 - __main__ - INFO -  validation dataset sampling rate casted to: 22050\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-30 11:04:55,296 - __main__ - INFO -  done extracting features for validation dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a936a923d1e2462fa48853a7e163f7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6cbf38cd7c4d239f7f76d8ba140e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([66, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([66]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = get_feature_extractor(args.model_name, args.train_dataset_mean, args.train_dataset_std)\n",
    "\n",
    "# creating train and validation datasets\n",
    "train_dataset_encoded = preprocess_data_for_training(dataset_path=train_path, sampling_rate=args.sampling_rate, feature_extractor=feature_extractor,\n",
    "                                                     fe_batch_size=args.fe_batch_size, dataset_name=\"train\", shuffle=True, extract_file_name=False)\n",
    "\n",
    "val_dataset_encoded = preprocess_data_for_training(dataset_path=val_path, sampling_rate=args.sampling_rate, feature_extractor=feature_extractor,\n",
    "                                                   fe_batch_size=args.fe_batch_size, dataset_name=\"validation\")\n",
    "\n",
    "# Download model from model hub\n",
    "model = ASTForAudioClassification.from_pretrained(args.model_name, num_labels=num_labels, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496dcea3-fe56-4783-87df-cf1e66692f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a675bc6-b182-4cf2-9e71-ae27fbde55f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Siamese_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.rnd = np.random.RandomState(0)\n",
    "\n",
    "        self.x_data = torch.tensor(train_dataset_encoded[:]['input_values'], dtype=torch.float32).to(device)\n",
    "        self.y_data = torch.tensor(train_dataset_encoded[:]['label'], dtype=torch.int64).to(device) \n",
    "\n",
    "        self.n = len(self.x_data)\n",
    "        self.singles = [k for k, v in Counter(self.y_data.tolist()).items() if v==1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "#TODO: create better triplet selection\n",
    "    def __getitem__(self, idx1):\n",
    "        y = self.y_data[idx1]\n",
    "        idx2 = self.rnd.randint(0,self.n-1)\n",
    "        if idx1 == idx2:\n",
    "            idx2 += 1\n",
    "        y2 = self.y_data[idx2]\n",
    "        idx3 = self.rnd.randint(0,self.n-1)\n",
    "\n",
    "        if y != y2:  # get two images with same label\n",
    "            while self.y_data[idx3] != y:\n",
    "                idx3 += 1\n",
    "                if idx3 == idx1 and not self.y_data[idx3] in self.singles:\n",
    "                    idx3 += 1\n",
    "                if idx3 == self.n: idx3 = 0\n",
    "            idx2, idx3 = idx3, idx2\n",
    "        else:  # get images different labels\n",
    "            while self.y_data[idx3] == y:\n",
    "                idx3 += 1\n",
    "                if idx3 == self.n: idx3 = 0\n",
    "\n",
    "        pixels1 = self.x_data[idx1]\n",
    "        label1 = self.y_data[idx1] \n",
    "        pixels2 = self.x_data[idx2]\n",
    "        label2 = self.y_data[idx2] \n",
    "        pixels3 = self.x_data[idx3]\n",
    "        label3 = self.y_data[idx3]\n",
    "        return (pixels1, label1, pixels2, label2, pixels3, label3)\n",
    "    \n",
    "    \n",
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.rnd = np.random.RandomState(0)\n",
    "\n",
    "        self.x_data = torch.tensor(train_dataset_encoded[:]['input_values'], dtype=torch.float32).to(device)\n",
    "        self.y_data = torch.tensor(train_dataset_encoded[:]['label'], dtype=torch.int64).to(device) \n",
    "\n",
    "        self.n = len(self.x_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x_data[index]\n",
    "        y = self.y_data[index]\n",
    "\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b3c5ab9-8a5b-4032-84c5-e02f7d688104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SiameseNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNet, self).__init__()  # pre 3.3 syntax\n",
    "\n",
    "        self.model = ASTForAudioClassification.from_pretrained(args.model_name, num_labels=num_labels, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n",
    "        self.model = self.model.audio_spectrogram_transformer\n",
    "        #self.model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "# TODO: check if we can just use AST model not ASTForAudioClassification\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output = self.model(torch.cat((x1, x2, x3), 0)).pooler_output\n",
    "        return torch.tensor_split(output, 3)\n",
    "    \n",
    "    \n",
    "class ClassificationNet(torch.nn.Module):\n",
    "    def __init__(self, siamese_net):\n",
    "        super(ClassificationNet, self).__init__()  # pre 3.3 syntax\n",
    "        self.model = ASTForAudioClassification.from_pretrained(args.model_name, num_labels=num_labels, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n",
    "        self.model = self.model.classifier\n",
    "        self.siamese_net = siamese_net\n",
    "# TODO: check other classification architecture\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.siamese_net(x).pooler_output\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390e753-e7c7-4b05-bfbe-297874f8c483",
   "metadata": {},
   "source": [
    "### TRAIN SIAMESE PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3121bb11-b85e-4769-bdcd-0cc90b782a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "batch_size = 2\n",
    "max_epochs = 10\n",
    "ep_log_interval = 1\n",
    "lrn_rate = 2e-5\n",
    "\n",
    "\n",
    "print(\"\\nBegin MNIST Siamese network demo \")\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "train_ds = Siamese_Dataset()\n",
    "train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "#TODO: check what margin would be best\n",
    "\n",
    "# 2. create network\n",
    "print(\"\\nCreating Siamese network \")\n",
    "net = SiameseNet().to(device)\n",
    "\n",
    "# 3. train model\n",
    "\n",
    "triplet_loss = torch.nn.TripletMarginLoss(margin=10.0, p=2)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lrn_rate)\n",
    "\n",
    "print(\"\\nStarting training\")\n",
    "net.train()  # set mode\n",
    "for epoch in range(0, max_epochs):\n",
    "    ep_loss = 0  # for one full epoch\n",
    "    for (batch_idx, batch) in tqdm(enumerate(train_ldr)):\n",
    "        X1, y1, X2, y2, X3, y3 = batch\n",
    "        oupt1, oupt2, oupt3 = net(X1, X2, X3)\n",
    "\n",
    "        optimizer.zero_grad()       # reset gradients\n",
    "        loss_val = triplet_loss(oupt1, oupt2, oupt3)\n",
    "\n",
    "        ep_loss += loss_val.item()  # accumulate loss\n",
    "        loss_val.backward()         # compute grads\n",
    "        optimizer.step()            # update weights\n",
    "    if epoch % ep_log_interval == 0:\n",
    "        print(\"epoch = %4d  |  loss = %10.4f\" % (epoch, ep_loss))\n",
    "    print(\"Done \") \n",
    "print(\"\\nEnd MNIST Siamese demo \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69acb22d-b161-4bc6-9d65-4fcd5329329c",
   "metadata": {},
   "source": [
    "##### ONLY FOR SAVING/LOADING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4689a05f-1332-46b3-bf15-9ed0887a457f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([66, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([66]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# net = SiameseNet().to(device)\n",
    "# import pickle\n",
    "\n",
    "# pickle.dump(net.model, open('model_siamese.pkl', 'wb'))\n",
    "# net = torch.load('model_siamese.pkl', map_location=torch.device('cpu'))\n",
    "# net.model = pickle.load(open('model_siamese.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9cfc2d-945b-4ca4-abaf-55eba03d9c0f",
   "metadata": {},
   "source": [
    "### TRAIN CLASSIFICATION PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25ab42e4-e2ab-4284-801d-48fe26dc52c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([66, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([66]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "44it [00:47,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    0  |  loss =   163.8971\n",
      "Done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:48,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    1  |  loss =    74.2935\n",
      "Done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:50,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    2  |  loss =    25.5594\n",
      "Done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:53,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    3  |  loss =     8.2624\n",
      "Done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:53,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    4  |  loss =     3.2833\n",
      "Done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:53,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    5  |  loss =     3.5265\n",
      "Done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:53,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    6  |  loss =     1.4283\n",
      "Done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:53,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    7  |  loss =     0.9449\n",
      "Done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:53,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    8  |  loss =     0.7021\n",
      "Done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:53,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    9  |  loss =     0.5776\n",
      "Done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lrn_rate = 2e-5\n",
    "max_epochs = 10\n",
    "ep_log_interval = 1\n",
    "batch_size_class = 4\n",
    "\n",
    "train_class_ds = ClassificationDataset()\n",
    "train_class_ldr = torch.utils.data.DataLoader(train_class_ds, batch_size=batch_size_class, shuffle=True)\n",
    "\n",
    "classification_net = ClassificationNet(net.model).to(device) #pass net.model - siamese model\n",
    "optimizer_class = torch.optim.Adam(classification_net.parameters(), lr=lrn_rate)\n",
    "categorical_crossentropy = torch.nn.functional.cross_entropy\n",
    "\n",
    "for epoch in range(0, max_epochs):\n",
    "    ep_loss = 0  # for one full epoch\n",
    "    for (batch_idx, batch) in tqdm(enumerate(train_class_ldr)):\n",
    "        X, y = batch\n",
    "        output = classification_net(X)\n",
    "\n",
    "        optimizer_class.zero_grad()       # reset gradients\n",
    "        loss_val = categorical_crossentropy(output, y)\n",
    "\n",
    "        ep_loss += loss_val.item()  # accumulate loss\n",
    "        loss_val.backward()         # compute grads\n",
    "        optimizer_class.step()            # update weights\n",
    "    if epoch % ep_log_interval == 0:\n",
    "        print(\"epoch = %4d  |  loss = %10.4f\" % (epoch, ep_loss))\n",
    "    print(\"Done \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c9df79-5b4a-4a29-90c5-b43242ee5a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_predictions2(examples: torch.Tensor, \n",
    "                     model: torch.nn.Module, \n",
    "                     device,\n",
    "                     labels: torch.Tensor = None):\n",
    "    model = model.to(device)\n",
    "    examples = torch.tensor(examples, dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64).to(device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(examples)\n",
    "    predicted_class_id = [str(torch.argmax(item).item()) for item in logits]\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(-1, 66), labels.to(device).view(-1), reduction=\"none\")\n",
    "        loss = loss.view(len(examples), -1).cpu().numpy()\n",
    "\n",
    "        return {'predicted_class_id': predicted_class_id, 'loss': loss, 'logits': logits}\n",
    "    else:\n",
    "        return {'predicted_class_id': predicted_class_id}\n",
    "\n",
    "    \n",
    "def compute_metrics(pred, labels):\n",
    "    # labels = pred.label_ids\n",
    "    # preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average=\"macro\")\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bdee6a6-0119-4f5c-a274-1f5f718a520f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dataset_encoded2 = val_dataset_encoded.map(lambda x: make_predictions2(x['input_values'], classification_net, device, x['label']), batched=True, batch_size=4, remove_columns=\"input_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49cd158c-1676-46f7-9a87-debcc339bf13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5909090909090909,\n",
       " 'f1': 0.4952380952380952,\n",
       " 'precision': 0.4532828282828283,\n",
       " 'recall': 0.5909090909090909}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics([int(i) for i in val_dataset_encoded2[:]['predicted_class_id']], val_dataset_encoded2[:]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e81cd-f7b0-4819-97f9-419232db13d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562c364-49ca-440f-a73b-83b98d24f486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "GDSC (custom-gdsc/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:292159885427:image-version/custom-gdsc/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:292159885427:studio-lifecycle-config/clean-trash"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
