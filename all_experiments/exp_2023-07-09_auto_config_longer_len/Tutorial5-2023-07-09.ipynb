{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97b856e-d267-44fa-9666-d4cf436470f6",
   "metadata": {},
   "source": [
    "# Tutorial 5 - Getting better results with SageMaker training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb3c7b-b471-48a0-a64b-07aee90ccf22",
   "metadata": {},
   "source": [
    "In the last tutorial, we trained and evaluated the first model on a small sample of data. While it is possible to train a model within [Sagemaker Studio](https://www.youtube.com/watch?v=uQc8Itd4UTs&list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz) it's better to use a Sagemaker training job instead. Sagemaker training jobs have several advantages over a normal notebook. To name few of them:\n",
    "\n",
    "- provide you with a nice overview of all the training you ran\n",
    "- automatically store the results of a training run (metrics, [logs](https://console.aws.amazon.com/cloudwatch) and models)\n",
    "- do not automatically shut down after a few hours (which we enabled in the notebooks)\n",
    "- allows running multiple training jobs in parallel (if you have sufficient GPUs allocated)\n",
    "\n",
    "In this notebook, we will convert the approach from tutorial 4 into a Sagemaker training job and train our model on the full data set. During the training, the logs are sent to [Cloudwatch](https://console.aws.amazon.com/cloudwatch) - AWS monitoring service. After the training is completed, the model is saved and automatically uploaded to S3. From there we'll retrieve the model and evaluate it.\n",
    "\n",
    "**NOTE: We will NOT need a GPU for this tutorial notebook. Pick a non-GPU instance type to save costs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30592f3-1c00-45b7-a6a6-66691ddeebff",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to import required libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40745116-f24d-4e57-8ce4-6e39d3badec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6729fb30-c66a-4a08-b618-250967909456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da96692-6164-49fc-881f-663b022185a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/data/experiments/exp_2023-07-09_auto_config_longer_len'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d1e3ca-df58-418e-8b76-770c30dfcefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys                                                                             # Python system library needed to load custom functions\n",
    "import numpy as np                                                                     # for performing calculations on numerical arrays\n",
    "import pandas as pd                                                                    # home of the DataFrame construct, _the_ most important object for Data Science\n",
    "import seaborn as sns                                                                  # additional plotting library\n",
    "import matplotlib.pyplot as plt                                                        # allows creation of insightful plots\n",
    "import os                                                                              # for changing the directory\n",
    "\n",
    "import sagemaker                                                                       # dedicated sagemaker library to execute training jobs\n",
    "import boto3                                                                           # for interacting with S3 buckets\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace                                           # for executing the trainig jobs\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score             # tools to understand how our model is performing\n",
    "\n",
    "#sys.path.append('')                                                               # Add the source directory to the PYTHONPATH. This allows to import local functions and modules.\n",
    "from config import DEFAULT_BUCKET, DEFAULT_REGION  \n",
    "from gdsc_utils import create_encrypted_bucket, download_and_extract_model, PROJECT_DIR # functions to create S3 buckets and to help with downloading models. Importing our root directory\n",
    "from gdsc_eval import plot_confusion_matrix                                             # function for creating confusion matrix                                     # importing the bucket name that contains data for the challenge and the default region\n",
    "os.chdir(PROJECT_DIR)                                                                   # changing our directory to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548c87e9-b9cb-434e-b708-d2d1e715048b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature_extractor = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", mean=dataset_mean, std=dataset_std, num_mel_bins=256)\n",
    "\n",
    "# config = ASTConfig.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", \n",
    "#                                    num_labels=num_labels, \n",
    "#                                    label2id=label2id, \n",
    "#                                    id2label=id2label,\n",
    "#                                    num_mel_bins = 256)\n",
    "# model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", config = config, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa3cff6-ab5b-4670-ba36-6b426d5a7c7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running the Training Script\n",
    "\n",
    "The training job will run on a virtual machine (called an instance) in the AWS cloud. An overview of all your training jobs can be found in the [AWS console](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs) (you may have to change the region) after you logged into your account. You can also navigate directly to *Amazon SageMaker > Training > Training jobs* and click on the name of the latest training job.\n",
    "\n",
    "To start, we need to set the name of our experiment. Keep in mind that every experiment should have a unique name. Since we advise you to use a separate Python script for each training, we'll use the name of the training script as the name of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1973311c-bf2b-4e95-9fad-ff0bfb51d342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseline-ast-train'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry_point = 'baseline_ast_train.py'\n",
    "exp_name = entry_point.split('.')[0].replace('_', '-')  # AWS does not allow . and _ as experiment names\n",
    "exp_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a4a5fa-e822-44b9-aa47-e8833b514ad8",
   "metadata": {},
   "source": [
    "Next, we need to define the AWS settings for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc19b0a2-0e01-40d4-be62-89c505fc3073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba884f95-09e4-40e9-af7a-f4c6e81e565e",
   "metadata": {},
   "source": [
    "We will also need to download the data to train a model. Luckily, Sagemaker has built-in functionality for this. \n",
    "Via the ```input_channels``` parameter we can specify multiple S3 locations. The contents are downloaded in the training job and made available under the provided name (dictionary key).\n",
    "In the example below, Sagemaker will download the complete content of the training data bucket, store it on the instance and, save its location in an environment variable called ```SM_CHANNEL_DATA```.<br>\n",
    "You can define up to 20 different channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19516277-f734-4365-bf88-b06fa8c78df3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gdsc.data.public.us-east-1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0110d-7500-48ce-aad4-c4ee3f70f013",
   "metadata": {},
   "source": [
    "Below is the required structure in our **S3_bucket** that needs to be followed to create the HuggingFace dataset without any issues. The *metadata.csv* file should contain two columns:\n",
    "- file_name - the name of the file,\n",
    "- label - the ground truth label for that file.\n",
    "\n",
    "As we do not have labels for the test set, we only include the *file_name* column in the *metadata.csv* in the test directory.\n",
    "\n",
    "```\n",
    "S3_bucket/\n",
    "    └── data/\n",
    "        |── labels.json\n",
    "        └── train/\n",
    "            |── train_file_1.wav\n",
    "            |── train_file_2.wav\n",
    "            |── ...\n",
    "            |── metadata.csv\n",
    "        └── val/\n",
    "            |── val_file_1.wav\n",
    "            |── val_file_2.wav\n",
    "            |── ...\n",
    "            |── metadata.csv\n",
    "        └── test/\n",
    "            |── test_file_1.wav\n",
    "            |── test_file_2.wav\n",
    "            |── ...\n",
    "            |── metadata.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71b55d6-3556-4612-8f06-0ed1091782e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_BUCKET = 'sagemaker-us-east-1-292159885427/original_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75e3e65c-7832-4e5e-9ce1-ebce1600a11c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 's3://sagemaker-us-east-1-292159885427/original_dataset'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_channels = {    \n",
    "    \"data\": f\"s3://{DEFAULT_BUCKET}\"    \n",
    "}\n",
    "input_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da305191-4387-46b6-b9a4-9ac9f9955ab6",
   "metadata": {},
   "source": [
    "We also specify where Sagemaker should store the results of the training job - i.e. the weights of the trained model. You will also see the link at the bottom of the training job overview page. If you already created a specific bucket for that purpose change ```s3_output_location``` variable to this bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76aa836b-0eac-45fd-a928-f8175567c516",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-292159885427/baseline-ast-train'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to create our own s3 bucket if it doesn't exist yet:\n",
    "sagemaker_bucket = f\"sagemaker-{DEFAULT_REGION}-{account_id}\"\n",
    "create_encrypted_bucket(sagemaker_bucket)\n",
    "\n",
    "s3_output_location = f\"s3://{sagemaker_bucket}/{exp_name}\"\n",
    "s3_output_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d88063a-d310-4871-8cf9-7a39abeeca7a",
   "metadata": {},
   "source": [
    "Using the <b>argparse</b> module in our training script, we can define the parameters that will be passed to the script. These parameters can include hyperparameters sent by the user, which are passed as command-line arguments to the script. Please go to the *baseline_ast_train.py* file located in the *src* and inspect the lines from 114 to 130, to see the whole list of possible arguments that you can pass to the script we've prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de7da21-feda-4c38-9dca-e977f7186839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    \"epochs\":10,                                                   # number of training epochs\n",
    "    \"patience\":3,                                                  # early stopping - how many epoch without improvement will stop the training\n",
    "    \"train_batch_size\":4,                                          # training batch size\n",
    "    \"model_name\":\"MIT/ast-finetuned-audioset-10-10-0.4593\",        # name of the pretrained model from HuggingFace\n",
    "    \"train_dir\":\"train\",                                           # folder name with training data\n",
    "    \"val_dir\":\"val\",                                               # folder name with validation data\n",
    "    \"test_dir\":\"test\",                                             # folder name with test data\n",
    "    \"train_dataset_mean\":--8.213229492272689,                       # mean value of spectrograms of our resampled data \n",
    "    \"train_dataset_std\":4.2898735494510225,                         # standard deviation value of spectrograms of our resampled data\n",
    "    \"learning_rate\":2e-5,\n",
    "    \"num_mel_bins\":128,\n",
    "    \"max_length\":1200  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d738979-5f62-46fc-913d-a647080b73fc",
   "metadata": {},
   "source": [
    "Finally, we need to specify which metrics we want Sagemaker to automatically track. For this, we need to set up [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) that will be applied to the logs.\n",
    "The corresponding values will then be stored and made visible in the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb052cbf-8e3b-4061-a5ea-1ea6ed233edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "    {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'learning_rate', 'Regex': \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_loss', 'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy', 'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_f1', 'Regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_precision', 'Regex': \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_recall', 'Regex': \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f798b487-0627-4c34-8192-604b1180021b",
   "metadata": {},
   "source": [
    "After all the necessary preparations, we are now ready to define a *SageMaker Estimator* which handles end-to-end Amazon SageMaker training and deployment tasks. While there are multiple available Estimators in SageMaker, we need to use the one that is compatible with our training script, so in our case, we will use the ```HuggingFace``` estimator.\n",
    "\n",
    "- SageMaker takes care of installing all the necessary libraries for the training job, but if there are any additional modules you want to use in the job, you can define them in ```src/requirements.txt```.\n",
    "- All the steps defined in our entry point will run during the training jobs. Therefore, if you want to change anything in the training process, you should define it in the Python script.\n",
    "- To start the job, we can call ```estimator.fit().``` With this command, the whole content of the src directory will be uploaded to the training instance. Therefore, if you want to use any additional functions, you should define them in src.\n",
    "- Note that interrupting the training job in the notebook won't stop it. If you don't want your job to finish, you have to stop it in the UI. Also, you can run only one job at a time.\n",
    "- When your training job is completed, your model will be stored as a .tar file in S3. The ```s3_output_location``` determines the location, and you can find it in the folder ```<your-training-job>/output```. You can download your model from there and test it locally. If there is no output folder, make sure to check the logs in the AWS console, as your training job may have failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ecb685-813d-426f-8499-2b6e2cab5df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INSTANCE_TYPE = \"ml.g4dn.xlarge\" # CHANGED !!! \n",
    "SOURCE_DIR = \"exp_2023-07-09_auto_config_longer_len\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cee902e6-d4c3-4d68-a1a6-b4f47021cb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = '954362353459.dkr.ecr.us-east-1.amazonaws.com/sm-training-custom:latest'\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=entry_point,                # fine-tuning script to use in training job\n",
    "    source_dir=SOURCE_DIR,           # directory where fine-tuning script is stored. This directory will be downloaded to training instance\n",
    "    instance_type=INSTANCE_TYPE,         # instance type - ml.g4dn.xlarge is a GPU instance so the training will be faster \n",
    "    output_path = s3_output_location,       # outputbucket to store our model after training\n",
    "    instance_count=1,                       # number of instances. We are limited to 1 instance\n",
    "    role=role,                              # IAM role used in training job to acccess AWS resources (S3)\n",
    "    image_uri = image_uri,                  # passing our custom image with the required libraries\n",
    "    py_version=\"py310\",                     # Python version \n",
    "    hyperparameters=hyperparameters,        # hyperparameters to use in training job\n",
    "    metric_definitions = metric_definitions # metrics we want to extract from logs. It will be visible in SageMaker training job UI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4b54a-eae4-4f60-bd0e-6f3a3d9d68e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "After we created the estimator, we will need to call the *fit* method to start the training job. As this might take a while, we can set ```wait=False``` so our notebook will not wait for the training job to finish and we can continue working, but for the sake of the tutorial let's set it to ```True```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce4bdcc7-ba86-494e-8310-91373b10e4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sm-training-custom-2023-07-09-19-08-18-991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-09 19:08:19 Starting - Starting the training job.........\n",
      "2023-07-09 19:09:43 Starting - Preparing the instances for training......\n",
      "2023-07-09 19:10:41 Downloading - Downloading input data..................\n",
      "2023-07-09 19:13:51 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:04,351 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:04,368 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:04,377 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:04,383 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:05,729 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:05,753 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:05,777 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:05,786 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"data\": \"/opt/ml/input/data/data\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"learning_rate\": 2e-05,\n",
      "        \"max_length\": 1200,\n",
      "        \"model_name\": \"MIT/ast-finetuned-audioset-10-10-0.4593\",\n",
      "        \"num_mel_bins\": 128,\n",
      "        \"patience\": 3,\n",
      "        \"test_dir\": \"test\",\n",
      "        \"train_batch_size\": 4,\n",
      "        \"train_dataset_mean\": 8.213229492272689,\n",
      "        \"train_dataset_std\": 4.2898735494510225,\n",
      "        \"train_dir\": \"train\",\n",
      "        \"val_dir\": \"val\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"data\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"sm-training-custom-2023-07-09-19-08-18-991\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-292159885427/sm-training-custom-2023-07-09-19-08-18-991/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"baseline_ast_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"baseline_ast_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"learning_rate\":2e-05,\"max_length\":1200,\"model_name\":\"MIT/ast-finetuned-audioset-10-10-0.4593\",\"num_mel_bins\":128,\"patience\":3,\"test_dir\":\"test\",\"train_batch_size\":4,\"train_dataset_mean\":8.213229492272689,\"train_dataset_std\":4.2898735494510225,\"train_dir\":\"train\",\"val_dir\":\"val\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=baseline_ast_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"data\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"data\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=baseline_ast_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-292159885427/sm-training-custom-2023-07-09-19-08-18-991/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"data\":\"/opt/ml/input/data/data\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"learning_rate\":2e-05,\"max_length\":1200,\"model_name\":\"MIT/ast-finetuned-audioset-10-10-0.4593\",\"num_mel_bins\":128,\"patience\":3,\"test_dir\":\"test\",\"train_batch_size\":4,\"train_dataset_mean\":8.213229492272689,\"train_dataset_std\":4.2898735494510225,\"train_dir\":\"train\",\"val_dir\":\"val\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"data\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"sm-training-custom-2023-07-09-19-08-18-991\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-292159885427/sm-training-custom-2023-07-09-19-08-18-991/source/sourcedir.tar.gz\",\"module_name\":\"baseline_ast_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"baseline_ast_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--learning_rate\",\"2e-05\",\"--max_length\",\"1200\",\"--model_name\",\"MIT/ast-finetuned-audioset-10-10-0.4593\",\"--num_mel_bins\",\"128\",\"--patience\",\"3\",\"--test_dir\",\"test\",\"--train_batch_size\",\"4\",\"--train_dataset_mean\",\"8.213229492272689\",\"--train_dataset_std\",\"4.2898735494510225\",\"--train_dir\",\"train\",\"--val_dir\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DATA=/opt/ml/input/data/data\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_LENGTH=1200\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=MIT/ast-finetuned-audioset-10-10-0.4593\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_MEL_BINS=128\u001b[0m\n",
      "\u001b[34mSM_HP_PATIENCE=3\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_DIR=test\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATASET_MEAN=8.213229492272689\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATASET_STD=4.2898735494510225\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DIR=train\u001b[0m\n",
      "\u001b[34mSM_HP_VAL_DIR=val\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 baseline_ast_train.py --epochs 10 --learning_rate 2e-05 --max_length 1200 --model_name MIT/ast-finetuned-audioset-10-10-0.4593 --num_mel_bins 128 --patience 3 --test_dir test --train_batch_size 4 --train_dataset_mean 8.213229492272689 --train_dataset_std 4.2898735494510225 --train_dir train --val_dir val\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:05,842 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\n",
      "2023-07-09 19:13:57 Training - Training image download completed. Training in progress.\u001b[34m2023-07-09 19:14:10,924 - __main__ - INFO - Torch version\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:10,924 - __main__ - INFO - 1.13.1+cu117\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:10,924 - __main__ - INFO - Torch sees CUDA?\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:10,992 - __main__ - INFO - True\u001b[0m\n",
      "\u001b[34mDownloading (…)rocessor_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)rocessor_config.json: 100%|██████████| 297/297 [00:00<00:00, 2.13MB/s]\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:11,075 - __main__ - INFO -  feature extractor loaded with dataset mean: 8.213229492272689 and standard deviation: 4.2898735494510225\u001b[0m\n",
      "\u001b[34mResolving data files:   0%|          | 0/1753 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mResolving data files: 100%|██████████| 1753/1753 [00:00<00:00, 20993.02it/s]\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-99e15f5c79a805bd/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1753 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1753/1753 [00:00<00:00, 90917.82it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1112 examples [00:00, 11085.70 examples/s]\u001b[0m\n",
      "\u001b[34mDataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-99e15f5c79a805bd/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 106.80it/s]\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:12,694 - __main__ - INFO -  loaded train dataset length is: 1752\u001b[0m\n",
      "\u001b[34m2023-07-09 19:14:12,701 - __main__ - INFO -  train dataset sampling rate casted to: 22050\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1752 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 32/1752 [00:08<07:11,  3.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▎         | 64/1752 [00:08<03:15,  8.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▌         | 96/1752 [00:10<02:36, 10.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 128/1752 [00:11<01:45, 15.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▉         | 160/1752 [00:13<01:40, 15.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 192/1752 [00:15<01:36, 16.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 224/1752 [00:16<01:22, 18.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 256/1752 [00:17<01:05, 22.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▋        | 288/1752 [00:19<01:19, 18.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 320/1752 [00:20<01:05, 21.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 352/1752 [00:21<00:56, 24.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 384/1752 [00:22<00:54, 25.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▎       | 416/1752 [00:25<01:13, 18.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 448/1752 [00:26<01:01, 21.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 480/1752 [00:30<01:27, 14.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 512/1752 [00:31<01:08, 18.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 544/1752 [00:32<01:05, 18.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 576/1752 [00:33<00:52, 22.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 608/1752 [00:36<01:07, 16.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 640/1752 [00:37<00:53, 20.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 672/1752 [00:38<00:51, 20.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|████      | 704/1752 [00:39<00:47, 22.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 736/1752 [00:41<00:45, 22.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 768/1752 [00:42<00:44, 22.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 800/1752 [00:43<00:40, 23.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 832/1752 [00:46<00:45, 20.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 864/1752 [00:46<00:35, 25.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 896/1752 [00:48<00:36, 23.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 928/1752 [00:49<00:35, 23.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▍    | 960/1752 [00:51<00:38, 20.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 992/1752 [00:52<00:32, 23.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 1024/1752 [00:53<00:29, 25.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 1056/1752 [00:55<00:35, 19.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 1088/1752 [00:57<00:32, 20.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 1120/1752 [00:58<00:29, 21.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 1152/1752 [00:59<00:25, 23.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 1184/1752 [01:00<00:20, 27.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 1216/1752 [01:01<00:17, 30.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████   | 1248/1752 [01:02<00:18, 26.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 1280/1752 [01:03<00:14, 32.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▍  | 1312/1752 [01:04<00:13, 32.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 1344/1752 [01:04<00:10, 37.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▊  | 1376/1752 [01:06<00:11, 32.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 1408/1752 [01:06<00:09, 34.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 1440/1752 [01:07<00:08, 37.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 1472/1752 [01:08<00:08, 34.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 1504/1752 [01:10<00:08, 29.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 1536/1752 [01:11<00:07, 28.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 1568/1752 [01:12<00:05, 31.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████▏| 1600/1752 [01:13<00:05, 28.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 1632/1752 [01:14<00:04, 28.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▍| 1664/1752 [01:15<00:02, 30.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 1696/1752 [01:16<00:01, 34.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▊| 1728/1752 [01:18<00:01, 22.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1752/1752 [01:20<00:00, 18.07 examples/s]\u001b[0m\n",
      "\u001b[34m2023-07-09 19:15:33,604 - __main__ - INFO -  done extracting features for train dataset\u001b[0m\n",
      "\u001b[34mResolving data files:   0%|          | 0/580 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mResolving data files: 100%|██████████| 580/580 [00:00<00:00, 102905.94it/s]\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-416dd2c9d6a1f53c/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/580 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 580/580 [00:00<00:00, 92382.04it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-416dd2c9d6a1f53c/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 378.68it/s]\u001b[0m\n",
      "\u001b[34m2023-07-09 19:15:34,163 - __main__ - INFO -  loaded validation dataset length is: 579\u001b[0m\n",
      "\u001b[34m2023-07-09 19:15:34,173 - __main__ - INFO -  validation dataset sampling rate casted to: 22050\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/579 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 32/579 [00:01<00:17, 30.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 64/579 [00:02<00:20, 25.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 96/579 [00:02<00:13, 35.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 128/579 [00:03<00:11, 40.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 160/579 [00:04<00:11, 36.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 192/579 [00:06<00:15, 24.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 224/579 [00:07<00:13, 25.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 256/579 [00:11<00:20, 15.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 288/579 [00:14<00:19, 14.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 320/579 [00:15<00:15, 16.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 352/579 [00:15<00:10, 21.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▋   | 384/579 [00:17<00:09, 21.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 416/579 [00:18<00:06, 23.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 448/579 [00:19<00:04, 27.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 480/579 [00:19<00:03, 30.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 512/579 [00:20<00:02, 31.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 544/579 [00:21<00:01, 33.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 576/579 [00:21<00:00, 43.11 examples/s]\u001b[0m\n",
      "\u001b[34m2023-07-09 19:15:56,169 - __main__ - INFO -  done extracting features for validation dataset\u001b[0m\n",
      "\u001b[34mResolving data files:   0%|          | 0/557 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mResolving data files: 100%|██████████| 557/557 [00:00<00:00, 226532.27it/s]\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-117b6a49f6b89b9b/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/557 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 557/557 [00:00<00:00, 92227.99it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-117b6a49f6b89b9b/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 393.28it/s]\u001b[0m\n",
      "\u001b[34m2023-07-09 19:15:56,679 - __main__ - INFO -  loaded test dataset length is: 556\u001b[0m\n",
      "\u001b[34m2023-07-09 19:15:56,688 - __main__ - INFO -  test dataset sampling rate casted to: 22050\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/556 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 32/556 [00:00<00:14, 37.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 64/556 [00:01<00:11, 41.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 96/556 [00:02<00:11, 41.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 128/556 [00:02<00:09, 45.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 160/556 [00:03<00:09, 40.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 192/556 [00:04<00:09, 39.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|████      | 224/556 [00:05<00:09, 36.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 256/556 [00:06<00:08, 36.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 288/556 [00:08<00:10, 26.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 320/556 [00:09<00:07, 32.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 352/556 [00:10<00:07, 27.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 384/556 [00:11<00:05, 32.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▍  | 416/556 [00:12<00:04, 33.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 448/556 [00:13<00:03, 32.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▋ | 480/556 [00:14<00:02, 29.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 512/556 [00:15<00:01, 31.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 544/556 [00:16<00:00, 30.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 556/556 [00:16<00:00, 30.67 examples/s]\u001b[0m\n",
      "\u001b[34m2023-07-09 19:16:13,465 - __main__ - INFO -  done extracting features for test dataset\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 137MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   9%|▉         | 31.5M/346M [00:00<00:01, 225MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  21%|██        | 73.4M/346M [00:00<00:00, 284MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  33%|███▎      | 115M/346M [00:00<00:00, 316MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  45%|████▌     | 157M/346M [00:00<00:00, 326MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  58%|█████▊    | 199M/346M [00:00<00:00, 345MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  70%|██████▉   | 241M/346M [00:00<00:00, 360MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  82%|████████▏ | 283M/346M [00:00<00:00, 368MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  94%|█████████▍| 325M/346M [00:00<00:00, 370MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin: 100%|██████████| 346M/346M [00:01<00:00, 344MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\u001b[0m\n",
      "\u001b[34m- audio_spectrogram_transformer.embeddings.position_embeddings: found shape torch.Size([1, 1214, 768]) in the checkpoint and torch.Size([1, 1430, 768]) in the model instantiated\u001b[0m\n",
      "\u001b[34m- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([66, 768]) in the model instantiated\u001b[0m\n",
      "\u001b[34m- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([66]) in the model instantiated\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\u001b[0m\n",
      "\u001b[34m- audio_spectrogram_transformer.embeddings.position_embeddings: found shape torch.Size([1, 1214, 768]) in the checkpoint and torch.Size([1, 1430, 768]) in the model instantiated\u001b[0m\n",
      "\u001b[34m- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([66, 768]) in the model instantiated\u001b[0m\n",
      "\u001b[34m- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([66]) in the model instantiated\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m2023-07-09 19:16:17,477 - __main__ - INFO -  starting training proccess for 10 epoch(s)\u001b[0m\n",
      "\u001b[34m{'loss': 4.2595, 'learning_rate': 1.995433789954338e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m{'loss': 3.9854, 'learning_rate': 1.990867579908676e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m{'loss': 3.6805, 'learning_rate': 1.9863013698630137e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m{'loss': 3.6062, 'learning_rate': 1.981735159817352e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m{'loss': 3.4493, 'learning_rate': 1.9771689497716898e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m{'loss': 2.999, 'learning_rate': 1.9726027397260276e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m{'loss': 2.9443, 'learning_rate': 1.9680365296803655e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m{'loss': 2.7362, 'learning_rate': 1.9634703196347033e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m{'loss': 2.211, 'learning_rate': 1.9589041095890412e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m{'loss': 2.395, 'learning_rate': 1.954337899543379e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m{'loss': 2.0333, 'learning_rate': 1.949771689497717e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m{'loss': 2.2462, 'learning_rate': 1.945205479452055e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m{'loss': 1.8284, 'learning_rate': 1.940639269406393e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m{'loss': 1.5911, 'learning_rate': 1.9360730593607308e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m{'loss': 1.749, 'learning_rate': 1.9315068493150686e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m{'loss': 2.0023, 'learning_rate': 1.9269406392694065e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m{'loss': 1.6861, 'learning_rate': 1.9223744292237447e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m{'loss': 1.6207, 'learning_rate': 1.9178082191780822e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m{'loss': 1.0768, 'learning_rate': 1.91324200913242e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m{'loss': 1.4567, 'learning_rate': 1.9086757990867582e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m{'loss': 1.3355, 'learning_rate': 1.904109589041096e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m{'loss': 1.7164, 'learning_rate': 1.899543378995434e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2607, 'learning_rate': 1.8949771689497718e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m{'loss': 1.4366, 'learning_rate': 1.8904109589041096e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2159, 'learning_rate': 1.8858447488584478e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m{'loss': 1.3404, 'learning_rate': 1.8812785388127853e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m{'loss': 0.9159, 'learning_rate': 1.8767123287671235e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m{'loss': 1.3209, 'learning_rate': 1.8721461187214614e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m{'loss': 1.4378, 'learning_rate': 1.8675799086757992e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2643, 'learning_rate': 1.863013698630137e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2812, 'learning_rate': 1.858447488584475e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m{'loss': 0.8364, 'learning_rate': 1.8538812785388128e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m{'loss': 0.9108, 'learning_rate': 1.849315068493151e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m{'loss': 1.1397, 'learning_rate': 1.8447488584474885e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m{'loss': 0.913, 'learning_rate': 1.8401826484018267e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6868, 'learning_rate': 1.8356164383561645e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m{'loss': 0.9414, 'learning_rate': 1.8310502283105024e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m{'loss': 0.9097, 'learning_rate': 1.8264840182648402e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m{'loss': 1.2655, 'learning_rate': 1.821917808219178e-05, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m{'loss': 0.878, 'learning_rate': 1.8173515981735163e-05, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m{'loss': 0.7305, 'learning_rate': 1.812785388127854e-05, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m{'loss': 0.7482, 'learning_rate': 1.808219178082192e-05, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m{'loss': 0.9937, 'learning_rate': 1.80365296803653e-05, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[34m╭───────────────────── Traceback (most recent call last) ──────────────────────╮\u001b[0m\n",
      "\u001b[34m│ /opt/ml/code/baseline_ast_train.py:227 in <module>                           │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   224 │                                                                      │\u001b[0m\n",
      "\u001b[34m│   225 │   # Train the model                                                  │\u001b[0m\n",
      "\u001b[34m│   226 │   logger.info(f\" starting training proccess for {args.epochs} epoch( │\u001b[0m\n",
      "\u001b[34m│ ❱ 227 │   trainer.train()                                                    │\u001b[0m\n",
      "\u001b[34m│   228 │                                                                      │\u001b[0m\n",
      "\u001b[34m│   229 │   # Prepare predictions on the validation set for the purpose of err │\u001b[0m\n",
      "\u001b[34m│   230 │   logger.info(\" training job done. Preparing predictions for validat │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1633 in      │\u001b[0m\n",
      "\u001b[34m│ train                                                                        │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1630 │   │   inner_training_loop = find_executable_batch_size(             │\u001b[0m\n",
      "\u001b[34m│   1631 │   │   │   self._inner_training_loop, self._train_batch_size, args.a │\u001b[0m\n",
      "\u001b[34m│   1632 │   │   )                                                             │\u001b[0m\n",
      "\u001b[34m│ ❱ 1633 │   │   return inner_training_loop(                                   │\u001b[0m\n",
      "\u001b[34m│   1634 │   │   │   args=args,                                                │\u001b[0m\n",
      "\u001b[34m│   1635 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\u001b[0m\n",
      "\u001b[34m│   1636 │   │   │   trial=trial,                                              │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1994 in      │\u001b[0m\n",
      "\u001b[34m│ _inner_training_loop                                                         │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1991 │   │   │   │   self.control.should_training_stop = True              │\u001b[0m\n",
      "\u001b[34m│   1992 │   │   │                                                             │\u001b[0m\n",
      "\u001b[34m│   1993 │   │   │   self.control = self.callback_handler.on_epoch_end(args, s │\u001b[0m\n",
      "\u001b[34m│ ❱ 1994 │   │   │   self._maybe_log_save_evaluate(tr_loss, model, trial, epoc │\u001b[0m\n",
      "\u001b[34m│   1995 │   │   │                                                             │\u001b[0m\n",
      "\u001b[34m│   1996 │   │   │   if DebugOption.TPU_METRICS_DEBUG in self.args.debug:      │\u001b[0m\n",
      "\u001b[34m│   1997 │   │   │   │   if is_torch_tpu_available():                          │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2236 in      │\u001b[0m\n",
      "\u001b[34m│ _maybe_log_save_evaluate                                                     │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   2233 │   │   │   │   │   │   metric_key_prefix=f\"eval_{eval_dataset_name}\" │\u001b[0m\n",
      "\u001b[34m│   2234 │   │   │   │   │   )                                                 │\u001b[0m\n",
      "\u001b[34m│   2235 │   │   │   else:                                                     │\u001b[0m\n",
      "\u001b[34m│ ❱ 2236 │   │   │   │   metrics = self.evaluate(ignore_keys=ignore_keys_for_e │\u001b[0m\n",
      "\u001b[34m│   2237 │   │   │   self._report_to_hp_search(trial, self.state.global_step,  │\u001b[0m\n",
      "\u001b[34m│   2238 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m│   2239 │   │   if self.control.should_save:                                  │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2932 in      │\u001b[0m\n",
      "\u001b[34m│ evaluate                                                                     │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   2929 │   │   start_time = time.time()                                      │\u001b[0m\n",
      "\u001b[34m│   2930 │   │                                                                 │\u001b[0m\n",
      "\u001b[34m│   2931 │   │   eval_loop = self.prediction_loop if self.args.use_legacy_pred │\u001b[0m\n",
      "\u001b[34m│ ❱ 2932 │   │   output = eval_loop(                                           │\u001b[0m\n",
      "\u001b[34m│   2933 │   │   │   eval_dataloader,                                          │\u001b[0m\n",
      "\u001b[34m│   2934 │   │   │   description=\"Evaluation\",                                 │\u001b[0m\n",
      "\u001b[34m│   2935 │   │   │   # No point gathering the predictions if there are no metr │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3113 in      │\u001b[0m\n",
      "\u001b[34m│ evaluation_loop                                                              │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   3110 │   │   │   │   │   batch_size = observed_batch_size                  │\u001b[0m\n",
      "\u001b[34m│   3111 │   │   │                                                             │\u001b[0m\n",
      "\u001b[34m│   3112 │   │   │   # Prediction step                                         │\u001b[0m\n",
      "\u001b[34m│ ❱ 3113 │   │   │   loss, logits, labels = self.prediction_step(model, inputs │\u001b[0m\n",
      "\u001b[34m│   3114 │   │   │   inputs_decode = self._prepare_input(inputs[\"input_ids\"])  │\u001b[0m\n",
      "\u001b[34m│   3115 │   │   │                                                             │\u001b[0m\n",
      "\u001b[34m│   3116 │   │   │   if is_torch_tpu_available():                              │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3368 in      │\u001b[0m\n",
      "\u001b[34m│ prediction_step                                                              │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   3365 │   │   │   else:                                                     │\u001b[0m\n",
      "\u001b[34m│   3366 │   │   │   │   if has_labels or loss_without_labels:                 │\u001b[0m\n",
      "\u001b[34m│   3367 │   │   │   │   │   with self.compute_loss_context_manager():         │\u001b[0m\n",
      "\u001b[34m│ ❱ 3368 │   │   │   │   │   │   loss, outputs = self.compute_loss(model, inpu │\u001b[0m\n",
      "\u001b[34m│   3369 │   │   │   │   │   loss = loss.mean().detach()                       │\u001b[0m\n",
      "\u001b[34m│   3370 │   │   │   │   │                                                     │\u001b[0m\n",
      "\u001b[34m│   3371 │   │   │   │   │   if isinstance(outputs, dict):                     │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2677 in      │\u001b[0m\n",
      "\u001b[34m│ compute_loss                                                                 │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   2674 │   │   │   labels = inputs.pop(\"labels\")                             │\u001b[0m\n",
      "\u001b[34m│   2675 │   │   else:                                                         │\u001b[0m\n",
      "\u001b[34m│   2676 │   │   │   labels = None                                             │\u001b[0m\n",
      "\u001b[34m│ ❱ 2677 │   │   outputs = model(**inputs)                                     │\u001b[0m\n",
      "\u001b[34m│   2678 │   │   # Save past state if it exists                                │\u001b[0m\n",
      "\u001b[34m│   2679 │   │   # TODO: this needs to be fixed and made cleaner later.        │\u001b[0m\n",
      "\u001b[34m│   2680 │   │   if self.args.past_index >= 0:                                 │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194 in   │\u001b[0m\n",
      "\u001b[34m│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/models/audio_spectrogra │\u001b[0m\n",
      "\u001b[34m│ m_transformer/modeling_audio_spectrogram_transformer.py:581 in forward       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   578 │   │   \"\"\"                                                            │\u001b[0m\n",
      "\u001b[34m│   579 │   │   return_dict = return_dict if return_dict is not None else self │\u001b[0m\n",
      "\u001b[34m│   580 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│ ❱ 581 │   │   outputs = self.audio_spectrogram_transformer(                  │\u001b[0m\n",
      "\u001b[34m│   582 │   │   │   input_values,                                              │\u001b[0m\n",
      "\u001b[34m│   583 │   │   │   head_mask=head_mask,                                       │\u001b[0m\n",
      "\u001b[34m│   584 │   │   │   output_attentions=output_attentions,                       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194 in   │\u001b[0m\n",
      "\u001b[34m│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/models/audio_spectrogra │\u001b[0m\n",
      "\u001b[34m│ m_transformer/modeling_audio_spectrogram_transformer.py:500 in forward       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   497 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│   498 │   │   embedding_output = self.embeddings(input_values)               │\u001b[0m\n",
      "\u001b[34m│   499 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│ ❱ 500 │   │   encoder_outputs = self.encoder(                                │\u001b[0m\n",
      "\u001b[34m│   501 │   │   │   embedding_output,                                          │\u001b[0m\n",
      "\u001b[34m│   502 │   │   │   head_mask=head_mask,                                       │\u001b[0m\n",
      "\u001b[34m│   503 │   │   │   output_attentions=output_attentions,                       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194 in   │\u001b[0m\n",
      "\u001b[34m│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/models/audio_spectrogra │\u001b[0m\n",
      "\u001b[34m│ m_transformer/modeling_audio_spectrogram_transformer.py:352 in forward       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   349 │   │   │   │   │   layer_head_mask,                                   │\u001b[0m\n",
      "\u001b[34m│   350 │   │   │   │   )                                                      │\u001b[0m\n",
      "\u001b[34m│   351 │   │   │   else:                                                      │\u001b[0m\n",
      "\u001b[34m│ ❱ 352 │   │   │   │   layer_outputs = layer_module(hidden_states, layer_head │\u001b[0m\n",
      "\u001b[34m│   353 │   │   │                                                              │\u001b[0m\n",
      "\u001b[34m│   354 │   │   │   hidden_states = layer_outputs[0]                           │\u001b[0m\n",
      "\u001b[34m│   355                                                                        │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194 in   │\u001b[0m\n",
      "\u001b[34m│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/models/audio_spectrogra │\u001b[0m\n",
      "\u001b[34m│ m_transformer/modeling_audio_spectrogram_transformer.py:290 in forward       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   287 │   │   head_mask: Optional[torch.Tensor] = None,                      │\u001b[0m\n",
      "\u001b[34m│   288 │   │   output_attentions: bool = False,                               │\u001b[0m\n",
      "\u001b[34m│   289 │   ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]] │\u001b[0m\n",
      "\u001b[34m│ ❱ 290 │   │   self_attention_outputs = self.attention(                       │\u001b[0m\n",
      "\u001b[34m│   291 │   │   │   self.layernorm_before(hidden_states),  # in AST, layernorm │\u001b[0m\n",
      "\u001b[34m│   292 │   │   │   head_mask,                                                 │\u001b[0m\n",
      "\u001b[34m│   293 │   │   │   output_attentions=output_attentions,                       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194 in   │\u001b[0m\n",
      "\u001b[34m│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/models/audio_spectrogra │\u001b[0m\n",
      "\u001b[34m│ m_transformer/modeling_audio_spectrogram_transformer.py:229 in forward       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   226 │   │   head_mask: Optional[torch.Tensor] = None,                      │\u001b[0m\n",
      "\u001b[34m│   227 │   │   output_attentions: bool = False,                               │\u001b[0m\n",
      "\u001b[34m│   228 │   ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]] │\u001b[0m\n",
      "\u001b[34m│ ❱ 229 │   │   self_outputs = self.attention(hidden_states, head_mask, output │\u001b[0m\n",
      "\u001b[34m│   230 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│   231 │   │   attention_output = self.output(self_outputs[0], hidden_states) │\u001b[0m\n",
      "\u001b[34m│   232                                                                        │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194 in   │\u001b[0m\n",
      "\u001b[34m│ _call_impl                                                                   │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   1191 │   │   # this function, and just call forward.                       │\u001b[0m\n",
      "\u001b[34m│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\u001b[0m\n",
      "\u001b[34m│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\u001b[0m\n",
      "\u001b[34m│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\u001b[0m\n",
      "\u001b[34m│   1195 │   │   # Do not call functions when jit is used                      │\u001b[0m\n",
      "\u001b[34m│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\u001b[0m\n",
      "\u001b[34m│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│ /opt/conda/lib/python3.10/site-packages/transformers/models/audio_spectrogra │\u001b[0m\n",
      "\u001b[34m│ m_transformer/modeling_audio_spectrogram_transformer.py:154 in forward       │\u001b[0m\n",
      "\u001b[34m│                                                                              │\u001b[0m\n",
      "\u001b[34m│   151 │   │   # Take the dot product between \"query\" and \"key\" to get the ra │\u001b[0m\n",
      "\u001b[34m│   152 │   │   attention_scores = torch.matmul(query_layer, key_layer.transpo │\u001b[0m\n",
      "\u001b[34m│   153 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│ ❱ 154 │   │   attention_scores = attention_scores / math.sqrt(self.attention │\u001b[0m\n",
      "\u001b[34m│   155 │   │                                                                  │\u001b[0m\n",
      "\u001b[34m│   156 │   │   # Normalize the attention scores to probabilities.             │\u001b[0m\n",
      "\u001b[34m│   157 │   │   attention_probs = nn.functional.softmax(attention_scores, dim= │\u001b[0m\n",
      "\u001b[34m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[34mOutOfMemoryError: CUDA out of memory. Tried to allocate 5.85 GiB (GPU 0; 14.75 \u001b[0m\n",
      "\u001b[34mGiB total capacity; 8.49 GiB already allocated; 4.35 GiB free; 9.38 GiB reserved\u001b[0m\n",
      "\u001b[34min total by PyTorch) If reserved memory is >> allocated memory try setting \u001b[0m\n",
      "\u001b[34mmax_split_size_mb to avoid fragmentation.  See documentation for Memory \u001b[0m\n",
      "\u001b[34mManagement and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m2023-07-09 19:28:21,597 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-07-09 19:28:21,597 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-07-09 19:28:21,598 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2023-07-09 19:28:21,598 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"OutOfMemoryError: CUDA out of memory. Tried to allocate 5.85 GiB (GPU 0; 14.75\n",
      " GiB total capacity; 8.49 GiB already allocated; 4.35 GiB free; 9.38 GiB reserved\n",
      " in total by PyTorch) If reserved memory is >> allocated memory try setting\n",
      " max_split_size_mb to avoid fragmentation.  See documentation for Memory\n",
      " Management and PYTORCH_CUDA_ALLOC_CONF\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.10 baseline_ast_train.py --epochs 10 --learning_rate 2e-05 --max_length 1200 --model_name MIT/ast-finetuned-audioset-10-10-0.4593 --num_mel_bins 128 --patience 3 --test_dir test --train_batch_size 4 --train_dataset_mean 8.213229492272689 --train_dataset_std 4.2898735494510225 --train_dir train --val_dir val\"\u001b[0m\n",
      "\u001b[34m2023-07-09 19:28:21,598 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2023-07-09 19:28:38 Uploading - Uploading generated training model\n",
      "2023-07-09 19:28:38 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job sm-training-custom-2023-07-09-19-08-18-991: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"OutOfMemoryError: CUDA out of memory. Tried to allocate 5.85 GiB (GPU 0; 14.75\n GiB total capacity; 8.49 GiB already allocated; 4.35 GiB free; 9.38 GiB reserved\n in total by PyTorch) If reserved memory is >> allocated memory try setting\n max_split_size_mb to avoid fragmentation.  See documentation for Memory\n Management and PYTORCH_CUDA_ALLOC_CONF\"\nCommand \"/opt/conda/bin/python3.10 baseline_ast_train.py --epochs 10 --learning_rate 2e-05 --max_length 1200 --model_name MIT/ast-finetuned-audioset-10-10-0.4593 --num_mel_bins 128 --patience 3 --test_dir test --train_batch_size 4 --train_dataset_mean 8.213229492272689 --train_dataset_std 4.2898735494510225 --train_dir train --val_dir val\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:284\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:1198\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:2344\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2342\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:4630\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4610\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4611\u001b[0m \n\u001b[1;32m   4612\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:6492\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6489\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 6492\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   6494\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:6545\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   6540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   6541\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6542\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6543\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6544\u001b[0m     )\n\u001b[0;32m-> 6545\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6546\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6547\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6548\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6549\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job sm-training-custom-2023-07-09-19-08-18-991: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"OutOfMemoryError: CUDA out of memory. Tried to allocate 5.85 GiB (GPU 0; 14.75\n GiB total capacity; 8.49 GiB already allocated; 4.35 GiB free; 9.38 GiB reserved\n in total by PyTorch) If reserved memory is >> allocated memory try setting\n max_split_size_mb to avoid fragmentation.  See documentation for Memory\n Management and PYTORCH_CUDA_ALLOC_CONF\"\nCommand \"/opt/conda/bin/python3.10 baseline_ast_train.py --epochs 10 --learning_rate 2e-05 --max_length 1200 --model_name MIT/ast-finetuned-audioset-10-10-0.4593 --num_mel_bins 128 --patience 3 --test_dir test --train_batch_size 4 --train_dataset_mean 8.213229492272689 --train_dataset_std 4.2898735494510225 --train_dir train --val_dir val\", exit code: 1"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(input_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9677df1-ec12-471c-b953-7ad9c660b618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the model location to the filesystem so that we can use it later\n",
    "model_location = f'{s3_output_location}/{huggingface_estimator._hyperparameters[\"sagemaker_job_name\"]}/output/model.tar.gz'\n",
    "print(model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a22b14-1bc5-4f97-b05f-b01a9278f2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving the csv file under the appropriate location. Create the folder if it doesn't exist \n",
    "model_folder_path =f\"models/{huggingface_estimator._hyperparameters['sagemaker_job_name']}\"\n",
    "\n",
    "if not os.path.exists(model_folder_path):\n",
    "    os.makedirs(model_folder_path)\n",
    "    \n",
    "with open(f'{model_folder_path}/model_location.txt', 'w+') as f:\n",
    "    f.write(model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4a9cd-4b06-4bb7-8b25-0148a211cdbe",
   "metadata": {},
   "source": [
    "One important question that we haven't answered to far is *How can I see how the training is doing?* After all you need see if there are any issues or if the training is going well.\n",
    "This can be done as follows:\n",
    "\n",
    "1. Go to the AWS Console\n",
    "2. Search for AWS Sagemaker and go to the Service\n",
    "3. On the left panel click on Training -> Training Jobs\n",
    "4. Your training job should the one at the very top. Click it.\n",
    "5. The details page has a link *View logs* in the *Monitor* section (scroll down). Click this to see the logs. \n",
    "\n",
    "The default configuration for this tutorial will train your model for approximately 7 epochs. This should take less than 2 hours and give you an indication if an idea is working or not.\n",
    "Training for longer does not necessary help. **To save time and money we suggest to start with a few epochs and only train models for longer where you are certain that there is a significant benefit.** After all, you only have a limited budget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebf51b-3553-48e4-8f65-f884fd054615",
   "metadata": {},
   "source": [
    "## Key takeaways:\n",
    "\n",
    "- We utilize **SageMaker Training jobs** to train our model, which takes place on a separate virtual machine (instance). The steps outlined in the training script (baseline_ast_train.py) will be executed there. If you wish to alter the training process, modify the training script accordingly (and we'd suggest you to save it under a new name).\n",
    "- You can monitor the job's progress in the [*SageMaker Trainings UI*]('https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs'). In the **Monitoring** section, you can keep track of metrics during the training process.\n",
    "- If you want to stop the training, you must do so in the [*SageMaker Trainings UI*]('https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs'). Stopping the job in the notebook alone will not suffice.\n",
    "- You can alter the input data to use your own S3 bucket if you have created the dataset differently.\n",
    "- Keep in mind that you can experiment with the **hyperparameters** of the model.\n",
    "- Since we employ a GPU instance for the training process, bear in mind that we pay for each second of training. If you do not see any improvement in longer trainings (more epochs), focus on other tasks such as data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c3134-0d58-49d2-8b80-0aaa9c511b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A new submission with our newly trained model!\n",
    "\n",
    "After the training job is finished - which should take in our case less than 2 hours! - you can download the results of the training job.\n",
    "\n",
    "*Note: Check the AWS training job console to see the status of your training job*\n",
    "\n",
    "First we need to specify where the results where stored. We stored the model location to the local filesystem, we only need to read it. If that didn't work, make sure to check the Sagemaker Training section in the AWS console. The model location will look similar to this:```s3://sagemaker-us-east-1-192475648101/train/huggingface-pytorch-training-2023-05-05-13-44-42-427/output/model.tar.gz```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b43838-716c-47cf-823f-a8f40eb36854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the model location from the filesystem\n",
    "with open(f'{model_folder_path}/model_location.txt', 'r') as f:\n",
    "    model_location = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed3eec-008b-4ce3-8057-b40801d25b37",
   "metadata": {},
   "source": [
    "We prepared a custom function that downloads the results to our local directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc86cd-a683-41be-8dd0-8c960d90bcb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_model_dir = download_and_extract_model(model_uri=model_location, local_dir='models')\n",
    "local_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732e186-0d63-4999-97a1-4215208aa31b",
   "metadata": {},
   "source": [
    "To confirm that everything ran correctly, please navigate to the *models* directory. There, you should find the following files:\n",
    "\n",
    "- *checkpoint-###* folders\n",
    "- *prediction_val.csv*\n",
    "- *prediction_test.csv*\n",
    "- *model.tar.gz*\n",
    "\n",
    "The checkpoint folders contain the model's weights, which can be loaded to generate predictions from different epochs. In the training script, we set ```load_best_model_at_end``` to **True**, which ensures that the model is loaded with the best checkpoint based on the validation loss. After loading the checkpoint, we generated predictions for both the validation and test sets, which can be found in the same directory where you extracted the model.\n",
    "\n",
    "With everything set up, let's proceed to loading the test set predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1cbbaa-4e29-460c-8c96-ea305ec5cc91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = pd.read_csv(f'{local_model_dir}/prediction_test.csv', index_col = False)\n",
    "test_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf337940-786e-411f-8863-ed3e7cb6dd44",
   "metadata": {},
   "source": [
    "You can upload this file to the [GDSC website](https://gdsc.ce.capgemini.com/) what score did you get? Do you see a big difference between this model and the one trained on a small sample of data?\n",
    "\n",
    "Please keep in mind that the submission file must adhere to the following format:\n",
    "\n",
    "- A *file_name* column, indicating the file for which the prediction was generated\n",
    "- A *predicted_class_id* column, containing the predicted ID for that file.\n",
    "\n",
    "**If the submission file deviates from this format, there is a risk of encountering problems when running the evaluation function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ab41e-6ecc-4f81-8e84-f856f2426957",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Error analysis on Validation set\n",
    "\n",
    "Error analysis is an essential part of machine learning, as it helps us to identify and diagnose problems that may arise during the training and testing stages of a machine learning model. By analyzing the errors, we can gain insights into the performance of the model, and we can make informed decisions about how to improve its accuracy and effectiveness.\n",
    "\n",
    "There are several reasons why error analysis is necessary in machine learning and how can it help with improving model performance:\n",
    "\n",
    "- Identify the sources of errors: By analyzing the errors, we can identify the sources of errors, such as data quality issues. This helps us to understand where the model is failing and how we can improve it.\n",
    "\n",
    "- Improve model accuracy: Error analysis can help us to fine-tune the model by identifying which features or parameters are causing errors, and adjusting them to improve accuracy.\n",
    "\n",
    "- Evaluate model performance: Error analysis helps us to evaluate the performance of the model against the desired outcome. It also allows us to compare the performance of different models and select the best one for our use case.\n",
    "\n",
    "As we prepared predictions on the validation set we can use them for the error analysis. Let's give it a try! \n",
    "\n",
    "\n",
    "First of all we should load ```prediction_val.csv``` file and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3dff0a-bf49-41b8-8b18-a725fb24e769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_preds = pd.read_csv(f'{local_model_dir}/prediction_val.csv', index_col = False)\n",
    "val_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4e386-54e1-4b4e-960d-2e63167d1bf1",
   "metadata": {},
   "source": [
    "This dataframe contains two additional columns that will be useful for Error Analysis:\n",
    "\n",
    "- label - the ground truth label\n",
    "- loss - the degree to which the model's prediction differs from the actual class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d029a3-1ff0-4437-8009-3ff6b27feca7",
   "metadata": {},
   "source": [
    "Before looking in depth at the loss values let's use our function imported from the *gdsc_eval.py* module to generate a **Confusion matrix** plot, which will provide greater insight into the model's performance for specific classes. \n",
    "\n",
    "Note from the docstring in the *gdsc_eval.py* that the last argument tells us if we want to normalize the values of the matrix over the true labels, predicted labels, the whole population or just give absolute values. Try to plot the confusion matrix with different normalizations and see if you can understand what do they mean. If you feel like something is not clear - post a comment on the Team's channel and start a conversation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bd128-1174-4fb8-b1ed-98dbb360e586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(val_preds['label'], val_preds['predicted_class_id'], normalize=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d442c6-5912-4cf8-af1e-845dd8661820",
   "metadata": {},
   "source": [
    "While the majority of the classes are accurately classified, there are a few that remain misclassified. Additionally, the model appears to have difficulty distinguishing between certain classes, would you be able to identify them? Maybe they are similar to each other?\n",
    "\n",
    "We will now examine the losses for individual predictions. Generally speaking, the model's confidence in a given prediction increases as the loss decreases. Let's see which are the top 10 classes with the highest loss values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793df32-4e2f-4301-964d-ec04d0e11a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classes sorted by the highest sum of loss\n",
    "(\n",
    "    val_preds.groupby(\"label\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a74d02-7a7b-4172-9312-96c80ec2a559",
   "metadata": {},
   "source": [
    "Since we only have a limited number of examples for certain classes, it might be more meaningful to focus on the average loss value instead. This approach could provide a more accurate representation of the model's performance for those classes with minimal validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4254a3b-ba73-4100-8cec-4a68986a236c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classes sorted by highest mean of loss\n",
    "(\n",
    "    val_preds.groupby(\"label\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)\n",
    "    .sort_values(by=\"mean\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63580a9-248b-40f0-bb4a-a632f4466518",
   "metadata": {},
   "source": [
    "The above tables give us an idea which classes are the most problematic. To investigate this further, we suggest plotting the examples with the highest loss and some of the examples of the class that the files were missclassified with. Try to identify any patterns or issues within the data. It is possible that this could be related to the quality of the data, and further analysis may be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265af1e-6ced-4083-97ca-3cc0bd9bd818",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exercises:\n",
    "\n",
    "- Conduct your own Error Analysis based on the validation dataset predictions. Explore classes with the highest and lowest losses, and visualize spectrograms of these classes using the functions from the 2nd tutorial on EDA. Identify any classes that the model is consistently misclassifying and investigate whether these classes are similar to each other.\n",
    "- Experiment with different hyperparameters during the training process to see how they affect model performance.\n",
    "- Check out the [HuggingFace model Hub]('https://huggingface.co/models?pipeline_tag=audio-classification&sort=downloads'), where you can test out various audio classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8964f5-191e-41da-82ed-2fd7a2a8db60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# What's next?\n",
    "\n",
    "Phew! That was a long journey! From analyzing the metadata to getting first results. But the work is not yet done - honestly it has just started! You may wonder how to improve the results we have obtained. Well, there are few possibilities:\n",
    "\n",
    "## Different preprocessing\n",
    "\n",
    "Maybe it's worth to go back to the documentation of the [ASTFeatureExtractor](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor). There are some hyperparameters that you can tweak like *max_length* of the file taken as the input.\n",
    "\n",
    "## Data Augmentation\n",
    "\n",
    "In the gdsc docker image provided by us you will find libraries like [torchaudio](https://pytorch.org/audio/stable/index.html) and [librosa](https://librosa.org/doc/main/index.html). Thsoe serve you the purpose of augmenting your dataset with use of various preprocessing steps. Maybe after inspecting the missclassifications you will come up with different methods for tweaking some audio features of the arrays?\n",
    "\n",
    "Just please note that only the torchaudio library works with GPU, librosa will work only with CPU, which will make your training much slower.\n",
    "\n",
    "## Other models\n",
    "\n",
    "The AST model is not the only one out there. Try to inspect other architectures. Maybe one of them will be the winning one! Here are two example sources you can inspect (feel free to find more!):\n",
    "\n",
    "#### Hugging Face Model Hub\n",
    "\n",
    "The [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=audio-classification&sort=downloads) has a variety of different models. Try to inspect few of them.  \n",
    "\n",
    "#### Papers with code\n",
    "\n",
    "The Papers with code site offers benchmarks of different models on different datasets. Here is an example of [benchmarking models on AudioSet dataset](https://paperswithcode.com/sota/audio-classification-on-audioset). This may be a bit advanced, but you can try to implement the code found in the repositories for a model that gives high scores."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "GDSC (custom-gdsc/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:292159885427:image-version/custom-gdsc/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:292159885427:studio-lifecycle-config/clean-trash"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
