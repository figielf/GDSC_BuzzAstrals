{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b4329d-cc54-4f24-885d-061d04ffcff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a204d02-4336-473e-8f7e-f3ee1a7fd9c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f1fd1c-9c46-4a7e-b9b2-d0b246a71506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/data/experiments/exp_2023-07-07_CNN'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c302ebc1-502f-4922-a24e-ee4952681e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys                                                                             # Python system library needed to load custom functions\n",
    "import numpy as np                                                                     # for performing calculations on numerical arrays\n",
    "import pandas as pd                                                                    # home of the DataFrame construct, _the_ most important object for Data Science\n",
    "import seaborn as sns                                                                  # additional plotting library\n",
    "import matplotlib.pyplot as plt                                                        # allows creation of insightful plots\n",
    "import os                                                                              # for changing the directory\n",
    "\n",
    "import sagemaker                                                                       # dedicated sagemaker library to execute training jobs\n",
    "import boto3                                                                           # for interacting with S3 buckets\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace                                           # for executing the trainig jobs\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score             # tools to understand how our model is performing\n",
    "\n",
    "#sys.path.append('')                                                               # Add the source directory to the PYTHONPATH. This allows to import local functions and modules.\n",
    "from config import DEFAULT_BUCKET, DEFAULT_REGION  \n",
    "from gdsc_utils import create_encrypted_bucket, download_and_extract_model, PROJECT_DIR # functions to create S3 buckets and to help with downloading models. Importing our root directory\n",
    "from gdsc_eval import plot_confusion_matrix                                             # function for creating confusion matrix                                     # importing the bucket name that contains data for the challenge and the default region\n",
    "os.chdir(PROJECT_DIR)                                                                   # changing our directory to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b416f2fb-add1-41e7-97b9-3f5e72fe78f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging                                                    # module for displaying relevant information in the logs\n",
    "import sys                                                        # to access to some variables used or maintained by the interpreter \n",
    "import argparse                                                   # to parse arguments from passed in the hyperparameters\n",
    "import os                                                         # to manage environmental variables\n",
    "import json                                                       # to open the json file with labels\n",
    "from transformers import (                                        # required classes to perform the model training and implement early stopping\n",
    "    ASTFeatureExtractor, \n",
    "    ASTForAudioClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    EarlyStoppingCallback\n",
    ")                                    \n",
    "import torch                                                       # library to work with PyTorch tensors and to figure out if we have a GPU available\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, Audio, Dataset                  # required tools to create, load and process our audio dataset\n",
    "import pandas as pd                                                # home of the DataFrame construct, _the_ most important object for Data Science\n",
    "from preprocessing import preprocess_audio_arrays                  # functions to preprocess the dataset with ASTFeatureExtractor\n",
    "from gdsc_eval import compute_metrics, make_predictions            # functions to create predictions and evaluate them\n",
    "from typing import Optional                                        # for type hints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c34402-b677-4ea5-9820-d5fb434ddde4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e23bcb-1736-4d44-b987-5f27fc8dbd2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2afe2d57-d8df-45c0-ba6b-f0b05f4b0890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a1910ae-96ff-4fe6-b378-5c29d71d038c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# hyperparameters sent from our jupyter notebook are passed as command-line arguments to the script\n",
    "# preprocessing hyperparameters\n",
    "# parser.add_argument(\"--sampling_rate\", type=int, default=22050)                        # sampling rate to which we will cast audio files\n",
    "parser.add_argument(\"--sampling_rate\", type=int, default=16000)                        # sampling rate to which we will cast audio files\n",
    "parser.add_argument(\"--fe_batch_size\", type=int, default=32)                           # feature extractor batch size\n",
    "# parser.add_argument(\"--train_dataset_mean\", type=float, default=-8.076275929131292)                  # mean value of spectrograms of our data\n",
    "# parser.add_argument(\"--train_dataset_std\", type=float, default=3.984092920341275)    \n",
    "# standard deviation value of spectrograms of our resampled data\n",
    "parser.add_argument(\"--train_dataset_mean\", type=float, default=-8.141991150530815)                  # mean value of spectrograms of our data\n",
    "parser.add_argument(\"--train_dataset_std\", type=float, default=4.095692486358449)                   # standard deviation value of spectrograms of our resampled data\n",
    "\n",
    "# training hyperparameters\n",
    "parser.add_argument(\"--model_name\", type=str)                                          # name of the pretrained model from HuggingFace\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=2e-5)                       # learning rate\n",
    "parser.add_argument(\"--epochs\", type=int, default=4)                                   # number of training epochs \n",
    "parser.add_argument(\"--train_batch_size\", type=int, default=4)                        # training batch size\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=64)                         # evaluation batch size\n",
    "parser.add_argument(\"--patience\", type=int, default=2)                                 # early stopping - how many epoch without improvement will stop the training \n",
    "# parser.add_argument(\"--data_channel\", type=str, default=os.environ[\"SM_CHANNEL_DATA\"]) # directory where input data from S3 is stored\n",
    "parser.add_argument(\"--train_dir\", type=str, default=\"train\")                          # folder name with training data\n",
    "parser.add_argument(\"--val_dir\", type=str, default=\"val\")                              # folder name with validation data\n",
    "parser.add_argument(\"--test_dir\", type=str, default=\"test\")                            # folder name with test data\n",
    "# parser.add_argument(\"--output_dir\", type=str, default=os.environ['SM_MODEL_DIR'])      # output directory. This directory will be saved in the S3 bucket\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()                    # parsing arguments from the notebook\n",
    "\n",
    "args.model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "\n",
    "# train_path = f\"{args.data_channel}/{args.train_dir}\"   # directory of our training dataset on the instance\n",
    "# val_path = f\"{args.data_channel}/{args.val_dir}\"       # directory of our validation dataset on the instance\n",
    "# test_path = f\"{args.data_channel}/{args.test_dir}\"     # directory of our test dataset on the instance\n",
    "\n",
    "train_path = 'data/data_small/train'\n",
    "val_path = 'data/data_small/val'\n",
    "\n",
    "# train_path = '../data/train'\n",
    "# val_path = '../data/val'\n",
    "\n",
    "# experiments/data/data_small/train/Achetadomesticus_XC751747-dat009-001_edit1.wav\n",
    "# Set up logging which allows to print information in logs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.getLevelName(\"INFO\"),\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0935d0ea-1dde-41b6-bc65-e2ca10c5ae24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_extractor(model_name: str, \n",
    "                          train_dataset_mean: Optional[float] = None, \n",
    "                          train_dataset_std: Optional[float] = None) -> ASTFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Retrieves a feature extractor for audio signal processing.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained model to use.\n",
    "        train_dataset_mean (float, optional): The mean value of the training dataset. Defaults to None.\n",
    "        train_dataset_std (float, optional): The standard deviation of the training dataset. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        ASTFeatureExtractor: An instance of the ASTFeatureExtractor class.\n",
    "\n",
    "    \"\"\"\n",
    "    if all((train_dataset_mean, train_dataset_std)):\n",
    "        feature_extractor = ASTFeatureExtractor.from_pretrained(model_name, mean=train_dataset_mean, std=train_dataset_std, max_length=1024)\n",
    "        logger.info(f\" feature extractor loaded with dataset mean: {train_dataset_mean} and standard deviation: {train_dataset_std}\")\n",
    "    else:\n",
    "        feature_extractor = ASTFeatureExtractor.from_pretrained(model_name)\n",
    "        logger.info(\" at least one of the optional arguments (mean, std) is missing\")\n",
    "        logger.info(f\" feature extractor loaded with default dataset mean: {feature_extractor.mean} and standard deviation: {feature_extractor.std}\")\n",
    "        \n",
    "    return feature_extractor\n",
    "\n",
    "def preprocess_data_for_training(\n",
    "    dataset_path: str,\n",
    "    sampling_rate: int,\n",
    "    feature_extractor: ASTFeatureExtractor,\n",
    "    fe_batch_size: int,\n",
    "    dataset_name: str,\n",
    "    shuffle: bool = False,\n",
    "    extract_file_name: bool = True) -> Dataset:\n",
    "    \"\"\"\n",
    "    Preprocesses audio data for training.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): The path to the dataset.\n",
    "        sampling_rate (int): The desired sampling rate for the audio.\n",
    "        feature_extractor (ASTFeatureExtractor): The feature extractor to use for preprocessing.\n",
    "        fe_batch_size (int): The batch size for feature extraction.\n",
    "        dataset_name (str, optional): The name of the dataset. Defaults to None.\n",
    "        shuffle (bool, optional): Whether to shuffle the dataset. Defaults to False.\n",
    "        extract_file_name (bool, optional): Whether to extract paths from audio features. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dataset: The preprocessed dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"audiofolder\", data_dir=dataset_path).get('train') # loading the dataset\n",
    "    \n",
    "    # perform shuffle if specified\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(seed=43)\n",
    "        \n",
    "    logger.info(f\" loaded {dataset_name} dataset length is: {len(dataset)}\")\n",
    "\n",
    "    if extract_file_name:\n",
    "        remove_metadata = lambda x: x.endswith(\".wav\")\n",
    "        extract_file_name = lambda x: x.split('/')[-1]\n",
    "\n",
    "        dataset_paths = list(dataset.info.download_checksums.keys())\n",
    "        dataset_paths = list(filter(remove_metadata, dataset_paths))\n",
    "        dataset_paths = list(map(extract_file_name, dataset_paths))\n",
    "        dataset = dataset.add_column(\"file_name\", dataset_paths)\n",
    "\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "    \n",
    "    logger.info(f\" {dataset_name} dataset sampling rate casted to: {sampling_rate}\")\n",
    "\n",
    "    dataset_encoded = dataset.map(\n",
    "        lambda x: preprocess_audio_arrays(x, 'audio', 'array', feature_extractor),\n",
    "        remove_columns=\"audio\",\n",
    "        batched=True,\n",
    "        batch_size=fe_batch_size\n",
    "    )\n",
    "    \n",
    "    logger.info(f\" done extracting features for {dataset_name} dataset\")\n",
    "    \n",
    "    return dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8894b652-ac9f-4b78-8a9d-245f51e500cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(f'data/data_small/labels.json', 'r') as f:\n",
    "with open(f'../data/labels.json', 'r') as f:\n",
    "        labels = json.load(f)\n",
    "    \n",
    "# Create mapping from label to id and id to label\n",
    "label2id, id2label = dict(), dict()\n",
    "for k, v in labels.items():\n",
    "    label2id[k] = str(v)\n",
    "    id2label[str(v)] = k\n",
    "\n",
    "num_labels = len(label2id)  # define number of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7160574-bc13-4f07-8619-a1f2723db1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e401baa9a3c45e2b40b64289419adba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-07 14:27:27,658 - __main__ - INFO -  feature extractor loaded with dataset mean: -8.141991150530815 and standard deviation: 4.095692486358449\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b945048deed4e17bb054ade70938bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-4ea904e8a4f39112/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfc24d92b324069b12d1183689faacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f23dccd0bc44a98fd04915277643c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f8c730f2f6495b99167d5c00ec3836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-4ea904e8a4f39112/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e7f8cc652f4d56a4ffef225c9235e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-07 14:27:28,064 - __main__ - INFO -  loaded train dataset length is: 176\n",
      "2023-07-07 14:27:28,068 - __main__ - INFO -  train dataset sampling rate casted to: 16000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-07 14:27:44,611 - __main__ - INFO -  done extracting features for train dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc57a207bad40e0b833ae725a29cfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-99123276a0f5ab0b/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d6e3fc7c4a4f75b11bf75b84a82f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3d5d50bc9845789ad0299826323105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4eb784e00747c4a0f28c19dde6b7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-99123276a0f5ab0b/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f0d3bc81ac4294ae5e77582acc6d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-07 14:27:44,859 - __main__ - INFO -  loaded validation dataset length is: 66\n",
      "2023-07-07 14:27:44,865 - __main__ - INFO -  validation dataset sampling rate casted to: 16000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-07 14:27:49,805 - __main__ - INFO -  done extracting features for validation dataset\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = get_feature_extractor(args.model_name, args.train_dataset_mean, args.train_dataset_std)\n",
    "\n",
    "# creating train and validation datasets\n",
    "train_dataset_encoded = preprocess_data_for_training(dataset_path=train_path, sampling_rate=args.sampling_rate, feature_extractor=feature_extractor,\n",
    "                                                     fe_batch_size=args.fe_batch_size, dataset_name=\"train\", shuffle=True, extract_file_name=False)\n",
    "\n",
    "val_dataset_encoded = preprocess_data_for_training(dataset_path=val_path, sampling_rate=args.sampling_rate, feature_extractor=feature_extractor,\n",
    "                                                   fe_batch_size=args.fe_batch_size, dataset_name=\"validation\")\n",
    "\n",
    "# Download model from model hub\n",
    "# model = ASTForAudioClassification.from_pretrained(args.model_name, num_labels=num_labels, label2id=label2id, id2label=id2label, ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3387d1b7-ebcd-4bed-a794-ee440894a338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /root/.cache/torch/hub/main.zip\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\"pytorch/vision\", \"resnet50\", weights=\"IMAGENET1K_V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dec576cd-5acb-4165-a8b3-78b5354d8045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(in_features=2048, out_features=66, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6ff8198-0118-477c-8736-3bddb661f898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = X.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f33acbbf-1a45-485a-a949-ef8fbc9a34ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1024, 128])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.resize_(4,1,1024,128).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "672754fa-9663-458b-ae23-b4e8982ce33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = X.expand(4, 3, 1024, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b419331-f02c-4047-afc8-b374d590f77c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 1024, 128])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0e152a4-9d34-4067-bbf2-091af637e96f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0753,  0.0674,  0.0227, -0.0393,  0.0487,  0.0449, -0.0378, -0.0707,\n",
       "         -0.0217, -0.0634, -0.0316,  0.0090, -0.0179, -0.0762, -0.2220,  0.0435,\n",
       "         -0.0040, -0.0144, -0.1401,  0.0057,  0.1060,  0.0543, -0.0204, -0.0124,\n",
       "         -0.1530, -0.0149,  0.1607, -0.0720,  0.0120, -0.0610,  0.1477, -0.1113,\n",
       "         -0.0376, -0.0373, -0.1345, -0.0067, -0.0782,  0.0826, -0.0186,  0.0926,\n",
       "          0.1733,  0.0157, -0.0985,  0.0557, -0.0099,  0.1344, -0.0687,  0.1669,\n",
       "          0.0361, -0.0008,  0.0653, -0.0043, -0.0098,  0.0538,  0.0238,  0.1329,\n",
       "         -0.0583,  0.0429, -0.1120, -0.0645,  0.0906, -0.1164,  0.1710,  0.0218,\n",
       "         -0.0271, -0.0584],\n",
       "        [-0.1051,  0.1952, -0.1550, -0.0662,  0.1017, -0.0060, -0.0606, -0.0533,\n",
       "         -0.1076, -0.1157,  0.0262, -0.0280, -0.0487, -0.0177, -0.1300, -0.0295,\n",
       "         -0.0493, -0.0041,  0.0254, -0.0524,  0.1747,  0.1224, -0.0424, -0.0749,\n",
       "         -0.0706, -0.0290,  0.0958, -0.0695,  0.0086, -0.0809,  0.1500,  0.0413,\n",
       "         -0.0014, -0.0470, -0.1126,  0.0190, -0.1286,  0.0921,  0.0039,  0.0576,\n",
       "          0.0884,  0.1328, -0.0836,  0.1212,  0.0639,  0.0555, -0.1033,  0.1590,\n",
       "          0.0039,  0.0712,  0.0430,  0.0780, -0.1410,  0.0897, -0.0493,  0.1347,\n",
       "         -0.0745,  0.0469, -0.1780, -0.0189,  0.0951, -0.0543,  0.1760, -0.0024,\n",
       "          0.0393, -0.0660],\n",
       "        [-0.2037,  0.1364, -0.2089, -0.0458,  0.0626,  0.0711, -0.0932, -0.1303,\n",
       "         -0.0445,  0.0163,  0.0242,  0.1305, -0.0305, -0.0039, -0.2967, -0.3010,\n",
       "         -0.0411,  0.0769, -0.2030, -0.0856,  0.0383, -0.0467, -0.0612, -0.0029,\n",
       "          0.0585, -0.1106,  0.2560, -0.0169, -0.0705,  0.1061,  0.1989, -0.0971,\n",
       "          0.1728, -0.0325, -0.0303,  0.1691, -0.2545,  0.1187,  0.2245, -0.2147,\n",
       "          0.1271,  0.2403,  0.1780,  0.0877,  0.1704,  0.2239,  0.1401,  0.2801,\n",
       "         -0.0898,  0.0344,  0.0372,  0.0300, -0.0522,  0.1464, -0.1700,  0.0880,\n",
       "          0.0440, -0.0811, -0.0592, -0.0407,  0.1624, -0.1025,  0.1967,  0.0303,\n",
       "         -0.1620, -0.1645],\n",
       "        [-0.0130,  0.0621, -0.0801,  0.0344,  0.0986,  0.0340, -0.0296, -0.0562,\n",
       "         -0.0503, -0.1092, -0.0616,  0.0430, -0.0452,  0.0337, -0.1860, -0.0257,\n",
       "          0.0047,  0.0521, -0.0686, -0.0654,  0.1759,  0.0672, -0.0634,  0.0223,\n",
       "         -0.1080,  0.0415,  0.0912, -0.0027,  0.0048, -0.0482,  0.1393, -0.0729,\n",
       "          0.0310, -0.0281, -0.0557,  0.0722, -0.0266,  0.1742,  0.0399,  0.0714,\n",
       "          0.0716,  0.0202, -0.1065,  0.0957, -0.0218,  0.0573,  0.0141,  0.1001,\n",
       "          0.0194,  0.0129,  0.0447,  0.0117,  0.0118,  0.0669,  0.0024,  0.0867,\n",
       "         -0.0599,  0.0127, -0.0748, -0.0208,  0.0797, -0.0832,  0.1490, -0.0455,\n",
       "          0.0117, -0.0593]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e613811e-3cd2-4d66-a010-d92957ee0644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class CNNPretrain(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNPretrain, self).__init__()\n",
    "        self.model = torch.hub.load(\"pytorch/vision\", \"resnet50\", weights=\"IMAGENET1K_V2\")\n",
    "        self.model = list(self.model.children())\n",
    "        w = self.model[0].weight\n",
    "        self.model[0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=2, bias=False)\n",
    "        self.model[0].weight = nn.Parameter(torch.mean(w, dim=1, keepdim=True))\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "        self.model.fc = nn.Linear(in_features=2048, out_features=66, bias=True)\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        # out = self.model.extract_features(x, padding_mask=torch.zeros(x.shape).bool().to(device))[0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4186ec94-a85b-4430-9b50-84101ae13c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.rnd = np.random.RandomState(0)\n",
    "        \n",
    "        if train:\n",
    "            self.x_data = torch.tensor(train_dataset_encoded[:]['input_values'], dtype=torch.float32).to(device)\n",
    "            self.y_data = torch.tensor(train_dataset_encoded[:]['label'], dtype=torch.int64).to(device) \n",
    "        else:\n",
    "            self.x_data = torch.tensor(val_dataset_encoded[:]['input_values'], dtype=torch.float32).to(device)\n",
    "            self.y_data = torch.tensor(val_dataset_encoded[:]['label'], dtype=torch.int64).to(device) \n",
    "    \n",
    "        self.n = self.x_data.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x_data[index]\n",
    "        y = self.y_data[index]\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9789ff3-3cb6-4780-9218-b9e7e576639e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_predictions(examples: torch.Tensor, \n",
    "                     model: torch.nn.Module, \n",
    "                     device,\n",
    "                     labels: torch.Tensor = None):\n",
    "    \"\"\"\n",
    "    Generates predictions and loss values for a given batch of examples and labels with the use of a provided model.\n",
    "\n",
    "    Args:\n",
    "        examples (torch.Tensor): A tensor of shape (batch_size, sequence_length) containing input examples.\n",
    "        labels (torch.Tensor): A tensor of shape (batch_size,) containing ground-truth labels.\n",
    "        model (torch.nn.Module): A PyTorch model to use for making predictions.\n",
    "        device (str or torch.device): The device to use for running the model (e.g. 'cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing one or two keys: 'predicted_class_id' (always) and 'loss' (optional).\n",
    "        'predicted_class_id' is a list of strings representing the predicted class IDs for each example.\n",
    "        'loss' is a numpy array of shape (batch_size, num_labels) containing the loss values for each example.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(examples.to(device)).logits\n",
    "    predicted_class_id = [str(torch.argmax(item).item()) for item in logits]\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(-1, model.num_labels), labels.to(device).view(-1), reduction=\"none\")\n",
    "        loss = loss.view(len(examples), -1).cpu().numpy()\n",
    "\n",
    "        return {'predicted_class_id': predicted_class_id, 'loss': loss}\n",
    "    else:\n",
    "        return {'predicted_class_id': predicted_class_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cc5d5e62-f47b-4a2e-9561-d45b7a5ece47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred, labels):\n",
    "    # labels = pred.label_ids\n",
    "    # preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average=\"macro\")\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80f2773a-d0a4-4908-852e-2d977fdfed46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024, 128])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbdadc-5709-4d62-937d-9022ddb8600c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "94a5b0df-691b-4432-a124-fb210624386d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_105/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">4020275647.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_105/4020275647.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_105/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">4234159335.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_105/4234159335.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1304</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__setattr__</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1301 │   │   │   │   </span>modules[name] = value                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1302 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> modules <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> name <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> modules:                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1303 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1304 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">TypeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"cannot assign '{}' as child module '{}' \"</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1305 │   │   │   │   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"(torch.nn.Module or None expected)\"</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1306 │   │   │   │   │   │   │   │   │   </span>.format(torch.typename(value), name))                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1307 │   │   │   │   </span>modules[name] = value                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>cannot assign <span style=\"color: #008000; text-decoration-color: #008000\">'list'</span> as child module <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span> <span style=\"font-weight: bold\">(</span>torch.nn.Module or <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span> expected<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_105/\u001b[0m\u001b[1;33m4020275647.py\u001b[0m:\u001b[94m6\u001b[0m in \u001b[92m<module>\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_105/4020275647.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_105/\u001b[0m\u001b[1;33m4234159335.py\u001b[0m:\u001b[94m7\u001b[0m in \u001b[92m__init__\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_105/4234159335.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1304\u001b[0m in \u001b[92m__setattr__\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1301 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodules[name] = value                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1302 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melif\u001b[0m modules \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m name \u001b[95min\u001b[0m modules:                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1303 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m value \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1304 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mTypeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mcannot assign \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m as child module \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m \u001b[0m\u001b[33m\"\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1305 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m(torch.nn.Module or None expected)\u001b[0m\u001b[33m\"\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1306 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   \u001b[0m.format(torch.typename(value), name))                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1307 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodules[name] = value                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0mcannot assign \u001b[32m'list'\u001b[0m as child module \u001b[32m'model'\u001b[0m \u001b[1m(\u001b[0mtorch.nn.Module or \u001b[3;35mNone\u001b[0m expected\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "lrn_rate = 1e-3\n",
    "max_epochs = 10\n",
    "ep_log_interval = 1\n",
    "\n",
    "classification_net = CNNPretrain().to(device)\n",
    "\n",
    "optimizer_class = torch.optim.AdamW(classification_net.parameters(), lr=lrn_rate)\n",
    "categorical_crossentropy = torch.nn.functional.cross_entropy\n",
    "\n",
    "train_class_ds = ClassificationDataset(train=True)\n",
    "val_class_ds = ClassificationDataset(train=False)\n",
    "\n",
    "batch_size_class = 4\n",
    "train_class_ldr = torch.utils.data.DataLoader(train_class_ds, batch_size=batch_size_class, shuffle=True)\n",
    "val_class_ldr = torch.utils.data.DataLoader(val_class_ds, batch_size=batch_size_class, shuffle=True)\n",
    "\n",
    "\n",
    "for epoch in range(0, max_epochs):\n",
    "    ep_loss = 0  # for one full epoch\n",
    "    for (batch_idx, batch) in tqdm(enumerate(train_class_ldr)):\n",
    "        X, y = batch\n",
    "        #X = X.expand(4, 3, 1024, 128)\n",
    "        output = classification_net(X)\n",
    "\n",
    "        optimizer_class.zero_grad()      # reset gradients\n",
    "        loss_val = categorical_crossentropy(output, y)\n",
    "\n",
    "        ep_loss += loss_val.item()  # accumulate loss\n",
    "        loss_val.backward()         # compute grads\n",
    "        optimizer_class.step()            # update weights\n",
    "    if epoch % ep_log_interval == 0:\n",
    "        print(\"epoch = %4d  |  loss = %10.4f\" % (epoch, ep_loss))\n",
    "        val_dataset_encoded2 = val_dataset_encoded.map(lambda x: make_predictions(x['input_values'], classification_net, device, x['label']), batched=True, batch_size=4, remove_columns=\"input_values\")\n",
    "        print(compute_metrics([int(i) for i in val_dataset_encoded2[:]['predicted_class_id']], val_dataset_encoded2[:]['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ba9cb-a5f4-40c4-97b9-5572e4873e92",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33837c2f-20f8-4cd0-84f6-575d318a0c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class ClassificationDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.rnd = np.random.RandomState(0)\n",
    "        \n",
    "        # if train:\n",
    "        self.data = val_dataset_encoded\n",
    "        self.max_len = 1100000\n",
    "        # else:\n",
    "        #     self.x_data = torch.tensor(val_dataset_encoded[:]['input_values'], dtype=torch.float32).to(device)\n",
    "        #     self.y_data = torch.tensor(val_dataset_encoded[:]['label'], dtype=torch.int64).to(device) \n",
    "\n",
    "        # self.n = len(self.x_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.num_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        y = torch.tensor(data['label'], dtype=torch.int64).to(device) \n",
    "        x = torch.tensor(data['audio']['array'], dtype=torch.float32).to(device)\n",
    "        x = torch.nn.functional.pad(x, (0, self.max_len - x.shape[0]), value=0)\n",
    "\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d493ec-2488-4560-85fc-f2d01c4cc51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "classification_net = BEATsPretrain().to(device)\n",
    "classification_net.model = pickle.load(open('model_beats.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c6c15-aaaf-40b5-ae8c-c82555cf7d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_class_ds = ClassificationDatasetTest()\n",
    "\n",
    "batch_size_class = 1\n",
    "val_class_ds = torch.utils.data.DataLoader(val_class_ds, batch_size=batch_size_class, shuffle=True)\n",
    "\n",
    "results = []\n",
    "labels = []\n",
    "for (batch_idx, batch) in tqdm(enumerate(val_class_ds)):\n",
    "    X, y = batch\n",
    "    output = classification_net(X)\n",
    "    results.append(output)\n",
    "    labels.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce818a13-70ec-4847-88bd-620549843808",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics([int(i) for i in results], labels) # not tested yet. Probably return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb23bf-747a-4bea-ad9c-499109acc71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c6fbc-7448-4ebe-86a4-906ff3d49502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "GDSC (custom-gdsc/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:292159885427:image-version/custom-gdsc/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:292159885427:studio-lifecycle-config/clean-trash"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
