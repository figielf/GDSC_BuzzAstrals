{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b034b3-54af-4256-8757-789282e8984f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to import required libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d1e3ca-df58-418e-8b76-770c30dfcefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#line to render the plots under the code cell that created it\n",
    "%matplotlib inline\n",
    "import json  # for working with json files\n",
    "import sys  # Python system library needed to load custom functions\n",
    "import numpy as np  # for performing calculations on numerical arrays\n",
    "import pandas as pd  # home of the DataFrame construct, _the_ most important object for Data Science\n",
    "import torch  # library to work with PyTorch tensors and to figure out if we have a GPU available\n",
    "import os     # for changing the directory\n",
    "\n",
    "from datasets import load_dataset, Audio  # required tools to create, load and process our audio dataset\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, TrainingArguments, Trainer  # required classes to perform the model training\n",
    "\n",
    "sys.path.append('../..')  # add the source directory to the PYTHONPATH. This allows to import local functions and modules.\n",
    "from gdsc_utils import download_directory, PROJECT_DIR # function to download the needed files from the official GDSC s3 bucket and our root directory\n",
    "from config import DEFAULT_BUCKET  # S3 bucket with the GDSC data\n",
    "from preprocessing import calculate_stats, preprocess_audio_arrays  # functions to calculate dataset statistics and preprocess the dataset with ASTFeatureExtractor\n",
    "from gdsc_eval import make_predictions, compute_metrics  # functions to create predictions and evaluate them\n",
    "os.chdir(PROJECT_DIR) # changing our directory to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1e80315-b41c-4ffc-a580-64ff513244fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c3b823-c0cd-4222-8e60-30fd904a71ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ebde100-60ca-413a-b46c-11242a19918e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ee42b-b3ac-40d4-a5e8-05a18b809b3d",
   "metadata": {},
   "source": [
    "After having imported the required libraries it's about time to create a ðŸ¤— dataset object that will allow us to handle our audio files during preprocessing and training. This will be also the first \"proof\" for ease of use of the ðŸ¤— library.\n",
    "\n",
    "The ðŸ¤— datasets module has a neat way to load the audio data type with which we are working. The only thing we need is the paths to the folders with audio and metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b951f5-590f-4c41-9f4b-e2957ab097f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# paths for the train and validation datasets\n",
    "train_path = 'data/train'\n",
    "val_path = 'data/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0933d02-6e52-4da5-83ef-ec8776465361",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/data\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b5fe9-e5e8-4dee-98e8-efeadde5e95f",
   "metadata": {},
   "source": [
    "Let's see what is the structure of the metadata files stored in those paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "372700d0-cb3b-4b53-9d26-45bf8f5f07c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_meta_df = pd.read_csv(f\"{train_path}/metadata.csv\")\n",
    "val_meta_df = pd.read_csv(f\"{val_path}/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68902c04-0236-4d1e-96eb-85a87ebec1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roeselianaroeselii_XC751814-dat028-019_edit1.wav</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roeselianaroeselii_XC752367-dat006-010.wav</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yoyettacelis_GBIF2465208563_IN36000894_50988.wav</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gomphocerippusrufus_XC752285-dat001-045.wav</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phaneropteranana_XC755717-221013-Phaneroptera-...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  label\n",
       "0   Roeselianaroeselii_XC751814-dat028-019_edit1.wav     56\n",
       "1         Roeselianaroeselii_XC752367-dat006-010.wav     56\n",
       "2   Yoyettacelis_GBIF2465208563_IN36000894_50988.wav     64\n",
       "3        Gomphocerippusrufus_XC752285-dat001-045.wav     26\n",
       "4  Phaneropteranana_XC755717-221013-Phaneroptera-...     41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a9e697-5058-4f1b-a03f-6167f0a76430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atrapsaltacorticina_GBIF2901504947_IN62966536_...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chorthippusbrunneus_XC751398-dat022-008_edit5.wav</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Psaltodaplaga_GBIF3031797565_IN68469430_159997...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Omocestusviridulus_XC752267-dat013-003_edit2.wav</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Omocestusviridulus_XC752263-dat012-007_edit1.wav</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  label\n",
       "0  Atrapsaltacorticina_GBIF2901504947_IN62966536_...      3\n",
       "1  Chorthippusbrunneus_XC751398-dat022-008_edit5.wav     10\n",
       "2  Psaltodaplaga_GBIF3031797565_IN68469430_159997...     53\n",
       "3   Omocestusviridulus_XC752267-dat013-003_edit2.wav     39\n",
       "4   Omocestusviridulus_XC752263-dat012-007_edit1.wav     39"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e836316-808a-44cd-a311-cdeec4a8c7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c11280d91d45f5abb2607ab6340767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-9540d76c2719c5c2/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4a850c1083405789e94bd7259135f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb6e2ef7fb940ea8fbee64c7a1a87ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8497a8d6f8e4e7198d819d794f5f1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-9540d76c2719c5c2/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ceab9b1188448748f6784e9e8db0757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01731976daae48ac94af0286a2e635ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-a29ad9e5c4708aae/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156acf5ffd2243c982f0a3b46edb3663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a6aed9d81f416794c5d4ff81624562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2ca3b3fad34be1b3cb534c06bfb3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-a29ad9e5c4708aae/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c399c90dcc4d33a5ccfdf9d0ef48c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# our first interaction with Hugging Face datasets!\n",
    "train_dataset = load_dataset(\"audiofolder\", data_dir=train_path).get('train').shuffle(seed = 42)  # load the dataset and shuffle the examples\n",
    "val_dataset = load_dataset(\"audiofolder\", data_dir=val_path).get('train')                         # load the validation dataset. But why do we have \"get('train')\" at the end of the line? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865a1db-d193-4c41-a90e-ea8822797afc",
   "metadata": {},
   "source": [
    "Seems that the dataset was loaded. Let's inspect the train_dataset and val_dataset variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0826f6b-e262-44a7-81b9-6bd9b297936b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['audio', 'label'],\n",
       "     num_rows: 1752\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['audio', 'label'],\n",
       "     num_rows: 579\n",
       " }))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83cd29-48c6-4010-a4fd-ae64115b181b",
   "metadata": {},
   "source": [
    "So clearly we've created some kind of dataset object. We can see that it has two features: 'audio' and 'label'. Let's see if we can unpack a bit more this vague looking object and see what exactly the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bb7a738-33de-4349-a385-0e59e0ac8178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'audio': {'path': '/root/data/data/train/Galangalabeculata_GBIF1978446031_IN19058490_28357.wav',\n",
       "   'array': array([ 0.        ,  0.        ,  0.        , ..., -0.03515625,\n",
       "          -0.02563477, -0.02597046]),\n",
       "   'sampling_rate': 44100},\n",
       "  'label': 24},\n",
       " {'audio': {'path': '/root/data/data/val/Achetadomesticus_XC751734-dat001-055_edit1.wav',\n",
       "   'array': array([0.00054932, 0.00112915, 0.00067139, ..., 0.00094604, 0.00201416,\n",
       "          0.00128174]),\n",
       "   'sampling_rate': 44100},\n",
       "  'label': 0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0], val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "263e343b-6686-45d7-8e75-032f669bf736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_SAMPLING_RATE = 22050\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=MODEL_SAMPLING_RATE))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=MODEL_SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c11d5e8b-747e-4eeb-bae3-e228cdcf596f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'audio': Audio(sampling_rate=22050, mono=True, decode=True, id=None),\n",
       "  'label': Value(dtype='int64', id=None)},\n",
       " {'audio': Audio(sampling_rate=22050, mono=True, decode=True, id=None),\n",
       "  'label': Value(dtype='int64', id=None)})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.info.features, val_dataset.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac0b1e80-e32d-4a83-a8a8-d8a77ecbc0a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da5c3ceee66427cae3590d85124902a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_extractor_stats = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", do_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f639555b-3aec-4834-bf6f-0651793d3791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_dataset = train_dataset.map(lambda x: calculate_stats(x, audio_field='audio', array_field='array', feature_extractor=feature_extractor_stats), batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ed9c4d-54fb-429f-a7b8-a21bcfc98d17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_mean = -8.141991150530815\n",
    "dataset_std = 4.095692486358449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9c74ec7-15f8-4e6e-b09d-7a79da334d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", mean=dataset_mean, std=dataset_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caba4f17-21ca-4e2c-ac7a-acb44f9fb6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1752 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_encoded = train_dataset.map(lambda x: preprocess_audio_arrays(x, audio_field='audio', \n",
    "                                                                            array_field='array', \n",
    "                                                                            feature_extractor=feature_extractor), remove_columns=\"audio\", batched=True, batch_size=2)\n",
    "val_dataset_encoded = val_dataset.map(lambda x: preprocess_audio_arrays(x, audio_field='audio', \n",
    "                                                                        array_field='array', \n",
    "                                                                        feature_extractor=feature_extractor), remove_columns=\"audio\", batched=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2cbd7e5-c0b2-4489-b300-0d5b53565c28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['label', 'input_values'],\n",
       "     num_rows: 1752\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['label', 'input_values'],\n",
       "     num_rows: 579\n",
       " }))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_encoded, val_dataset_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c666f-8a10-4a12-a4e4-fdf9668399c9",
   "metadata": {},
   "source": [
    "# Fine-tuning the AST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d71295e0-2565-4f7f-8ae4-beb99bf74579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/labels.json', 'r') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62fee10c-cfe0-43fd-aefe-36b43a9bc0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Achetadomesticus': 0,\n",
       " 'Aleetacurvicosta': 1,\n",
       " 'Atrapsaltacollina': 2,\n",
       " 'Atrapsaltacorticina': 3,\n",
       " 'Atrapsaltaencaustica': 4,\n",
       " 'Barbitistesyersini': 5,\n",
       " 'Bicoloranabicolor': 6,\n",
       " 'Chorthippusalbomarginatus': 7,\n",
       " 'Chorthippusapricarius': 8,\n",
       " 'Chorthippusbiguttulus': 9,\n",
       " 'Chorthippusbrunneus': 10,\n",
       " 'Chorthippusmollis': 11,\n",
       " 'Chorthippusvagans': 12,\n",
       " 'Chrysochraondispar': 13,\n",
       " 'Cicadaorni': 14,\n",
       " 'Clinopsaltaautumna': 15,\n",
       " 'Conocephalusdorsalis': 16,\n",
       " 'Conocephalusfuscus': 17,\n",
       " 'Cyclochilaaustralasiae': 18,\n",
       " 'Decticusverrucivorus': 19,\n",
       " 'Diceroproctaeugraphica': 20,\n",
       " 'Ephippigerdiurnus': 21,\n",
       " 'Eumodicogryllusbordigalensis': 22,\n",
       " 'Eupholidopteraschmidti': 23,\n",
       " 'Galangalabeculata': 24,\n",
       " 'Gampsocleisglabra': 25,\n",
       " 'Gomphocerippusrufus': 26,\n",
       " 'Gomphocerussibiricus': 27,\n",
       " 'Gryllusbimaculatus': 28,\n",
       " 'Grylluscampestris': 29,\n",
       " 'Leptophyespunctatissima': 30,\n",
       " 'Melanogryllusdesertus': 31,\n",
       " 'Metriopterabrachyptera': 32,\n",
       " 'Myrmeleotettixmaculatus': 33,\n",
       " 'Nemobiussylvestris': 34,\n",
       " 'Neotibicenpruinosus': 35,\n",
       " 'Oecanthuspellucens': 36,\n",
       " 'Omocestuspetraeus': 37,\n",
       " 'Omocestusrufipes': 38,\n",
       " 'Omocestusviridulus': 39,\n",
       " 'Phaneropterafalcata': 40,\n",
       " 'Phaneropteranana': 41,\n",
       " 'Pholidopteraaptera': 42,\n",
       " 'Pholidopteragriseoaptera': 43,\n",
       " 'Pholidopteralittoralis': 44,\n",
       " 'Platycleisalbopunctata': 45,\n",
       " 'Platypleuracfcatenata': 46,\n",
       " 'Platypleuraplumosa': 47,\n",
       " 'Platypleurasp10': 48,\n",
       " 'Platypleurasp12cfhirtipennis': 49,\n",
       " 'Platypleurasp13': 50,\n",
       " 'Popplepsaltaaeroides': 51,\n",
       " 'Popplepsaltanotialis': 52,\n",
       " 'Psaltodaplaga': 53,\n",
       " 'Pseudochorthippusmontanus': 54,\n",
       " 'Pseudochorthippusparallelus': 55,\n",
       " 'Roeselianaroeselii': 56,\n",
       " 'Ruspolianitidula': 57,\n",
       " 'Stauroderusscalaris': 58,\n",
       " 'Stenobothruslineatus': 59,\n",
       " 'Stenobothrusstigmaticus': 60,\n",
       " 'Tettigoniacantans': 61,\n",
       " 'Tettigoniaviridissima': 62,\n",
       " 'Tylopsislilifolia': 63,\n",
       " 'Yoyettacelis': 64,\n",
       " 'Yoyettarepetens': 65}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "608808b4-3598-4f3f-9263-4b24c873678a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label2id, id2label = dict(), dict()\n",
    "for k, v in labels.items():\n",
    "    label2id[k] = str(v)\n",
    "    id2label[str(v)] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bfec984-cb9b-46b7-9827-209ba2559e08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Achetadomesticus': '0',\n",
       " 'Aleetacurvicosta': '1',\n",
       " 'Atrapsaltacollina': '2',\n",
       " 'Atrapsaltacorticina': '3',\n",
       " 'Atrapsaltaencaustica': '4',\n",
       " 'Barbitistesyersini': '5',\n",
       " 'Bicoloranabicolor': '6',\n",
       " 'Chorthippusalbomarginatus': '7',\n",
       " 'Chorthippusapricarius': '8',\n",
       " 'Chorthippusbiguttulus': '9',\n",
       " 'Chorthippusbrunneus': '10',\n",
       " 'Chorthippusmollis': '11',\n",
       " 'Chorthippusvagans': '12',\n",
       " 'Chrysochraondispar': '13',\n",
       " 'Cicadaorni': '14',\n",
       " 'Clinopsaltaautumna': '15',\n",
       " 'Conocephalusdorsalis': '16',\n",
       " 'Conocephalusfuscus': '17',\n",
       " 'Cyclochilaaustralasiae': '18',\n",
       " 'Decticusverrucivorus': '19',\n",
       " 'Diceroproctaeugraphica': '20',\n",
       " 'Ephippigerdiurnus': '21',\n",
       " 'Eumodicogryllusbordigalensis': '22',\n",
       " 'Eupholidopteraschmidti': '23',\n",
       " 'Galangalabeculata': '24',\n",
       " 'Gampsocleisglabra': '25',\n",
       " 'Gomphocerippusrufus': '26',\n",
       " 'Gomphocerussibiricus': '27',\n",
       " 'Gryllusbimaculatus': '28',\n",
       " 'Grylluscampestris': '29',\n",
       " 'Leptophyespunctatissima': '30',\n",
       " 'Melanogryllusdesertus': '31',\n",
       " 'Metriopterabrachyptera': '32',\n",
       " 'Myrmeleotettixmaculatus': '33',\n",
       " 'Nemobiussylvestris': '34',\n",
       " 'Neotibicenpruinosus': '35',\n",
       " 'Oecanthuspellucens': '36',\n",
       " 'Omocestuspetraeus': '37',\n",
       " 'Omocestusrufipes': '38',\n",
       " 'Omocestusviridulus': '39',\n",
       " 'Phaneropterafalcata': '40',\n",
       " 'Phaneropteranana': '41',\n",
       " 'Pholidopteraaptera': '42',\n",
       " 'Pholidopteragriseoaptera': '43',\n",
       " 'Pholidopteralittoralis': '44',\n",
       " 'Platycleisalbopunctata': '45',\n",
       " 'Platypleuracfcatenata': '46',\n",
       " 'Platypleuraplumosa': '47',\n",
       " 'Platypleurasp10': '48',\n",
       " 'Platypleurasp12cfhirtipennis': '49',\n",
       " 'Platypleurasp13': '50',\n",
       " 'Popplepsaltaaeroides': '51',\n",
       " 'Popplepsaltanotialis': '52',\n",
       " 'Psaltodaplaga': '53',\n",
       " 'Pseudochorthippusmontanus': '54',\n",
       " 'Pseudochorthippusparallelus': '55',\n",
       " 'Roeselianaroeselii': '56',\n",
       " 'Ruspolianitidula': '57',\n",
       " 'Stauroderusscalaris': '58',\n",
       " 'Stenobothruslineatus': '59',\n",
       " 'Stenobothrusstigmaticus': '60',\n",
       " 'Tettigoniacantans': '61',\n",
       " 'Tettigoniaviridissima': '62',\n",
       " 'Tylopsislilifolia': '63',\n",
       " 'Yoyettacelis': '64',\n",
       " 'Yoyettarepetens': '65'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11bc1c24-2707-4a6c-a4cf-8aae9cb5f412",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'Achetadomesticus',\n",
       " '1': 'Aleetacurvicosta',\n",
       " '2': 'Atrapsaltacollina',\n",
       " '3': 'Atrapsaltacorticina',\n",
       " '4': 'Atrapsaltaencaustica',\n",
       " '5': 'Barbitistesyersini',\n",
       " '6': 'Bicoloranabicolor',\n",
       " '7': 'Chorthippusalbomarginatus',\n",
       " '8': 'Chorthippusapricarius',\n",
       " '9': 'Chorthippusbiguttulus',\n",
       " '10': 'Chorthippusbrunneus',\n",
       " '11': 'Chorthippusmollis',\n",
       " '12': 'Chorthippusvagans',\n",
       " '13': 'Chrysochraondispar',\n",
       " '14': 'Cicadaorni',\n",
       " '15': 'Clinopsaltaautumna',\n",
       " '16': 'Conocephalusdorsalis',\n",
       " '17': 'Conocephalusfuscus',\n",
       " '18': 'Cyclochilaaustralasiae',\n",
       " '19': 'Decticusverrucivorus',\n",
       " '20': 'Diceroproctaeugraphica',\n",
       " '21': 'Ephippigerdiurnus',\n",
       " '22': 'Eumodicogryllusbordigalensis',\n",
       " '23': 'Eupholidopteraschmidti',\n",
       " '24': 'Galangalabeculata',\n",
       " '25': 'Gampsocleisglabra',\n",
       " '26': 'Gomphocerippusrufus',\n",
       " '27': 'Gomphocerussibiricus',\n",
       " '28': 'Gryllusbimaculatus',\n",
       " '29': 'Grylluscampestris',\n",
       " '30': 'Leptophyespunctatissima',\n",
       " '31': 'Melanogryllusdesertus',\n",
       " '32': 'Metriopterabrachyptera',\n",
       " '33': 'Myrmeleotettixmaculatus',\n",
       " '34': 'Nemobiussylvestris',\n",
       " '35': 'Neotibicenpruinosus',\n",
       " '36': 'Oecanthuspellucens',\n",
       " '37': 'Omocestuspetraeus',\n",
       " '38': 'Omocestusrufipes',\n",
       " '39': 'Omocestusviridulus',\n",
       " '40': 'Phaneropterafalcata',\n",
       " '41': 'Phaneropteranana',\n",
       " '42': 'Pholidopteraaptera',\n",
       " '43': 'Pholidopteragriseoaptera',\n",
       " '44': 'Pholidopteralittoralis',\n",
       " '45': 'Platycleisalbopunctata',\n",
       " '46': 'Platypleuracfcatenata',\n",
       " '47': 'Platypleuraplumosa',\n",
       " '48': 'Platypleurasp10',\n",
       " '49': 'Platypleurasp12cfhirtipennis',\n",
       " '50': 'Platypleurasp13',\n",
       " '51': 'Popplepsaltaaeroides',\n",
       " '52': 'Popplepsaltanotialis',\n",
       " '53': 'Psaltodaplaga',\n",
       " '54': 'Pseudochorthippusmontanus',\n",
       " '55': 'Pseudochorthippusparallelus',\n",
       " '56': 'Roeselianaroeselii',\n",
       " '57': 'Ruspolianitidula',\n",
       " '58': 'Stauroderusscalaris',\n",
       " '59': 'Stenobothruslineatus',\n",
       " '60': 'Stenobothrusstigmaticus',\n",
       " '61': 'Tettigoniacantans',\n",
       " '62': 'Tettigoniaviridissima',\n",
       " '63': 'Tylopsislilifolia',\n",
       " '64': 'Yoyettacelis',\n",
       " '65': 'Yoyettarepetens'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e1ae494-a4a8-487a-9f82-b68ef9ba0d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(label2id)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a6de841-9ef2-40b7-be6e-c3181e559314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ee8d47c6bd45518bb2f2a832b713d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e9dc0f48c84be38e191421fd43a53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([66, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([66]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", \n",
    "                                                  num_labels=num_labels, \n",
    "                                                  label2id=label2id, \n",
    "                                                  id2label=id2label,\n",
    "                                                  ignore_mismatched_sizes=True\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51f22c27-c4c6-43b3-9bc2-bb6564af3afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    model.audio_spectrogram_transformer.encoder.layer[i].output.dropout = nn.Dropout(p=0.2, inplace=False)\n",
    "    model.audio_spectrogram_transformer.encoder.layer[i].attention.output.dropout = nn.Dropout(p=0.2, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f31da8f-e0b1-4fe6-b921-cf6a112390ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPOCHS = 8                        # variable defining number of training epochs\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='experiments/models/dropout',                # directory for saving model checkpoints and logs\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,      #number of epochs\n",
    "    per_device_train_batch_size=4,          # number of examples in batch for training\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=2,           # number of examples in batch for evaluation\n",
    "    evaluation_strategy=\"epoch\",            # makes evaluation at the end of each epoch\n",
    "    learning_rate=float(2e-5),              # learning rate\n",
    "    optim=\"adamw_torch\",                    # optimizer\n",
    "    logging_steps=1,                        # number of steps for logging the training process - one step is one batch\n",
    "    load_best_model_at_end=True,            # whether to load or not the best model at the end of the training\n",
    "    metric_for_best_model=\"eval_loss\",      # claiming that the best model is the one with the lowest loss on the validation set\n",
    "    save_strategy='epoch'                   # saving is done at the end of each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d879dce-d9e5-4f02-9a21-75bd87e2fd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                          # passing our model\n",
    "    args=training_args,                   # passing the above created arguments\n",
    "    compute_metrics=compute_metrics,      # passing the compute_metrics function that we imported from gdsc_eval module\n",
    "    train_dataset=train_dataset_encoded,  # passing the encoded train set\n",
    "    eval_dataset=val_dataset_encoded,     # passing the encoded validation set\n",
    "    tokenizer=feature_extractor           # passing the feature extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac005a-f814-4fb8-8881-c4f48c5c3215",
   "metadata": {},
   "source": [
    "Amazing! Now we did everything that was required to fine-tune the model. We can finally run the cell which will give us our \"version\" of the AST classifier, which is capable to distinguish different species from audio recordings. Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9ff56-f633-49a0-a8b3-19a40ec41c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='433' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 1:26:47, Epoch 7.89/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.704400</td>\n",
       "      <td>3.070569</td>\n",
       "      <td>0.265976</td>\n",
       "      <td>0.129460</td>\n",
       "      <td>0.149944</td>\n",
       "      <td>0.178152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.512700</td>\n",
       "      <td>2.136324</td>\n",
       "      <td>0.512953</td>\n",
       "      <td>0.337185</td>\n",
       "      <td>0.365311</td>\n",
       "      <td>0.390501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.049600</td>\n",
       "      <td>1.690186</td>\n",
       "      <td>0.583765</td>\n",
       "      <td>0.453687</td>\n",
       "      <td>0.521895</td>\n",
       "      <td>0.490047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.732400</td>\n",
       "      <td>1.407212</td>\n",
       "      <td>0.670121</td>\n",
       "      <td>0.574360</td>\n",
       "      <td>0.665089</td>\n",
       "      <td>0.582178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>1.368826</td>\n",
       "      <td>0.668394</td>\n",
       "      <td>0.570206</td>\n",
       "      <td>0.661960</td>\n",
       "      <td>0.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.608200</td>\n",
       "      <td>1.267480</td>\n",
       "      <td>0.706390</td>\n",
       "      <td>0.635224</td>\n",
       "      <td>0.716336</td>\n",
       "      <td>0.640188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>1.195891</td>\n",
       "      <td>0.728843</td>\n",
       "      <td>0.671081</td>\n",
       "      <td>0.741300</td>\n",
       "      <td>0.678630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='137' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [137/290 00:41 < 00:46, 3.29 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model_history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f74586-6f30-45fa-8837-df3021f1d3e7",
   "metadata": {},
   "source": [
    "Is it possible? We are performing better than the Random Forest model with only a fraction of data! Well, yes, that's possible, but remember that the validation set we are using here contains only 66 samples, so way less than the original set. If you want to really compare the model with the Random Forest we need to perform inference on the test set and send a submission. \n",
    "\n",
    "In the next section we will show you how to load the model from checkpoint and perform inference on the test set data.\n",
    "\n",
    "**Key insights:**\n",
    "* The ðŸ¤— models hub offers you a variety of models, BUT you should always remember to adjust them to your task - create appropriate mapping of labels to integers and specify the number of classes that you are working with\n",
    "* There is a number of parameters that define a training job - be mindful about how you are setting them and iterate over different values - this is called hyperparameter tuning\n",
    "* Fine-tuning such a big model on such a small sample is almost always a bad idea - big models require big data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81f943-e7a6-4f8c-8126-4e1324b1cd60",
   "metadata": {},
   "source": [
    "# Loading the model and doing inference on the test set\n",
    "\n",
    "If you look back at the *TrainingArguments* class you will see that we passed an *output_dir* argument that tells ðŸ¤— where to put the checkpoint with training metadata and model. We set it to *models/AST*, so let's use this directory to load the feature extractor and the model from the best checkpoint (note that this is not necessary, as we put in our *TrainingArguments* object an argument called *load_best_model_at_end* and we set it to *True*. This ensures that the variable *model* contains already the best one based on the metric of choice. We just wanted to show you how to load the model from other checkpoints in case you'd like to experiment). With ðŸ¤— library loading the checkpoint it's just a matter of two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3ed72b8-bb56-4b09-881b-d964e43a0da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = ASTFeatureExtractor.from_pretrained(\"models/AST/checkpoint-352\")\n",
    "model = ASTForAudioClassification.from_pretrained(\"models/AST/checkpoint-352\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86bfec-992e-4c67-9f37-c03660b49de5",
   "metadata": {},
   "source": [
    "Cool! Now let's get the test set data. We need to preprocess them in the same way as we did for the training. Let's start with simply loading the dataset and resample the audio arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed7bd1f6-8c94-48af-9dc2-e3af1b7e116c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a32d32376642a59b46c3eda7bd21bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-7efab534d5cbb83c/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0f82d2b53f4c41b66bad4c1083a53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b394f18caa4a10aaf480eb408684d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8888060bb7f4698897621dc65ac8597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-7efab534d5cbb83c/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8261ecc0b9a441a1a6a4fb005b86491f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_path = 'data/test'\n",
    "test_dataset = load_dataset(\"audiofolder\", data_dir=test_path).get('train')\n",
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=MODEL_SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff6b7717-17cc-4888-b2fa-bd901fb871f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio'],\n",
       "    num_rows: 556\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76e1e9a8-d864-488a-b08a-0c066cfaa602",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/root/data/data/test/0.wav',\n",
       "  'array': array([ 4.08071404e-14, -3.05009002e-13,  1.55307760e-13, ...,\n",
       "         -2.78414413e-03,  6.18211143e-02,  0.00000000e+00]),\n",
       "  'sampling_rate': 16000}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9bf69d-5ece-4e45-8ac3-e6529f4e3206",
   "metadata": {},
   "source": [
    "As we need the predictions file to have two columns - file_name and predicted_class_id, let's take care of extracting the paths for each data point and make it a feature called \"file_name\". \n",
    "\n",
    "For this purpose we'll use the metadata information from the dataset object that we just created.\n",
    "\n",
    "So let's get the paths of the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fcb418f-634a-4805-9282-8390aa368985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_paths = list(test_dataset.info.download_checksums.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a93024-47cf-41a5-97fd-9c9f5e7240ed",
   "metadata": {},
   "source": [
    "Let's inspect the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38632450-9617-42fe-afc3-1b01559f992d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/data/data/test/0.wav',\n",
       " '/root/data/data/test/1.wav',\n",
       " '/root/data/data/test/10.wav']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_paths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb031da0-211f-49ed-b509-4b099648feec",
   "metadata": {},
   "source": [
    "Great! We obtained the paths. One thing to note is that the test_paths variable contains also the metadata.csv file with file_names and labels (check it on your own!). We don't need it, so we will use a one-liner lambda function to extract only the items related to the audio files.\n",
    "\n",
    "Furthermore, we don't need the whole path - just the file names, so we will define another one-liner that gets the string after the last \"/\" character, which is exactly the file name.\n",
    "\n",
    "We will use the built-in filter and map methods that allow for applying a function on an Python iterable. With its help we will run the below defined lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b77dfbd3-e588-45c0-8804-a67a92a56a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_metadata = lambda x: x.endswith(\".wav\")\n",
    "extract_file_name = lambda x: x.split('/')[-1]\n",
    "\n",
    "test_paths = list(filter(remove_metadata, test_paths))\n",
    "test_paths = list(map(extract_file_name, test_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a2149-6999-47f7-9424-f6de512c4d4a",
   "metadata": {},
   "source": [
    "Let's see if the test_paths variable contains the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f2bcb72-50f6-44b3-a32e-3facba68cae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.wav', '1.wav', '10.wav']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_paths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69ad21-9e1c-4142-a9ac-1de52618e09d",
   "metadata": {},
   "source": [
    "Yes, we indeed have just the file names. Let's create a new column with the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86edd7e2-8913-4131-bd0e-40c869213ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.add_column(\"file_name\", test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358da5f6-93b9-4638-a7f0-8ab1dfb2afad",
   "metadata": {},
   "source": [
    "Let's inspect the newly created \"file_name\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2348b72c-ca57-496b-b61e-1e407780b736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'file_name'],\n",
       "    num_rows: 556\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02ce12fc-12b2-4991-9d6a-2558154d5a45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/root/data/data/test/0.wav',\n",
       "  'array': array([ 4.08071404e-14, -3.05009002e-13,  1.55307760e-13, ...,\n",
       "         -2.78414413e-03,  6.18211143e-02,  0.00000000e+00]),\n",
       "  'sampling_rate': 16000},\n",
       " 'file_name': '0.wav'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df797b9d-ba7d-4cb6-a763-c20f8c91e190",
   "metadata": {},
   "source": [
    "Amazing! We almost finished preprocessing the data. The last step is to pass the audio arrays through our feature extractor and set fromat of the \"input_values\" columns from numpy to torch, so that we can safely pass the spectrogram arrays through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f979930-9633-4166-89db-17aa89fe064f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset_encoded = test_dataset.map(lambda x: preprocess_audio_arrays(x, 'audio', 'array', feature_extractor), remove_columns=\"audio\", batched=True, batch_size = 2)\n",
    "test_dataset_encoded.set_format(type='torch', columns=['input_values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab46e0e-c9c3-4c44-b909-d8c181f41e33",
   "metadata": {},
   "source": [
    "Now let's inform the ðŸ¤— that we want to run the predicions on our GPU. To do this we need to define the *device* variable with help of the *PyTorch* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9d0524c-3e09-40cb-b510-262137ef5e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43121d97-728b-4998-9a62-586e95278958",
   "metadata": {},
   "source": [
    "Good, we are set up to perform the inference on the test set. Let's use the *make_predictions* function from our *gdsc_eval* modeule located in *src* directory. This time we will set the *batch_size* argument to 8, to avoid any out-of-memory issues. We are also dropping the \"input_values\" column, as we won't need it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42615997-ec47-4531-809b-99aca74c98f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset_encoded = test_dataset_encoded.map(lambda x: make_predictions(x['input_values'], model, device), batched=True, batch_size=8, remove_columns=\"input_values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6faee-e779-46c2-beea-f0c0d94afcb9",
   "metadata": {},
   "source": [
    "Let's now create a pandas dataframe from our ðŸ¤— dataset. We should see the columns file_name and predicted_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d177953-aa2b-4309-9ea5-ac80be298e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>predicted_class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.wav</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.wav</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.wav</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.wav</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101.wav</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name predicted_class_id\n",
       "0     0.wav                 14\n",
       "1     1.wav                 60\n",
       "2    10.wav                  9\n",
       "3   100.wav                 17\n",
       "4   101.wav                 56"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_encoded_df = test_dataset_encoded.to_pandas()\n",
    "test_dataset_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da28f9f1-13c7-4cf7-867b-51e2afca0af1",
   "metadata": {},
   "source": [
    "Great! Now we need to save the dataframe in a csv file and we are ready to send the predictions. We will save it in the directory of our model, to have everything in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbdb19f7-09cf-4012-a278-e5eb2f4253a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset_encoded_df.to_csv(\"models/AST/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85012d29-8b05-46dc-bf45-612958c72729",
   "metadata": {
    "tags": []
   },
   "source": [
    "And done! We have our CSV file with the predictions ready. Let's upload it via the challenge website and see our results!\n",
    "\n",
    "The score is way better than the one from Random Forest. Remember that in this tutorial we are using a much more powerful model, that was designed to work with audio data. But taking into account that the F1 metric ranges from 0 to 1, there is still some room for improvement. In the next tutorial, we will see how the model performs on the whole dataset. Then you will see what the model is really capable of! In the mean time, you can try to complete the exercises while making a coffee before the final tutorial.\n",
    "\n",
    "***\n",
    "**It is important that you name the columns exactly: **file_name** and **predicted_class_id**, otherwise your score won't appear on the leaderboard!**\n",
    "***\n",
    "\n",
    "**Exercise time:**\n",
    "\n",
    "The last exercise in this notebook is to \n",
    "* try to think how we could improve the model further apart from running it on the whole sample. What does your Data Science intuition tell you? Post your thoughts in the Team's channel and gain some recognition for your team! ðŸ˜ƒ\n",
    "* try also to use another model from the ðŸ¤— model hub. You will need to import other classes instead of ASTFeatureExtractor and ASTForAudioClassification. You will also need to change the string in the *from_pretrained* method and adjust the preprocessing. Sounds like a lot? Well, this is how we do Data Science! ðŸ˜ƒ\n",
    "\n",
    "REMINDER: After finishing your work remember to shut down the instance."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "GDSC (custom-gdsc/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:292159885427:image-version/custom-gdsc/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:292159885427:studio-lifecycle-config/clean-trash"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
